{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This note book is dedicated to evaluating agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -e disneyenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import disneyenv\n",
    "from stable_baselines3.common.callbacks import ProgressBarCallback,StopTrainingOnMaxEpisodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def visualize_performance(arr_reward,arr_travel,agent_name):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_figheight(5)\n",
    "    fig.set_figwidth(10)\n",
    "    fig.suptitle(\"Performance for \"+agent_name)\n",
    "    ax1.set_title(\"Reward\")\n",
    "    ax1.plot(np.arange(len(arr_reward)),arr_reward)\n",
    "    ax1.axhline(y = np.mean(arr_reward),ls = \"dashed\",color = \"red\")\n",
    "    ax1.set_xlabel(\"Days\")\n",
    "    ax1.set_ylabel(\"Reward\")\n",
    "    ax1.set_ylim([100,500])\n",
    "\n",
    "    ax2.set_title(\"Total travel time\")\n",
    "    ax2.plot(np.arange(len(arr_travel)),arr_travel,c = \"orange\")\n",
    "    ax2.axhline(y = np.mean(arr_travel),ls = \"dashed\",color = \"red\")\n",
    "    ax2.set_xlabel(\"Days\")\n",
    "    ax2.set_ylim([100,500])\n",
    "    ax2.set_ylabel(\"Travel Time\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deterministic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A deterministic agent\n",
    "env = gym.make(\"disneyenv/Disney-v0\",train = False)\n",
    "reward_arr = env.ridesinfo.popularity.apply(lambda x:  5 if type(x)!=str else env.reward_dict[x]).to_numpy()\n",
    "\n",
    "def get_action(obs,reward_arr=reward_arr):\n",
    "    indicies = np.where((obs[\"operationStatus\"]  + ~obs[\"pastActions\"]) == 2)[0]\n",
    "    if len(indicies) == 0:\n",
    "        return env.action_space.n-1\n",
    "    \n",
    "    wait_arr = obs[\"waitTime\"][indicies]\n",
    "    reward_arr = reward_arr[indicies]\n",
    "    tmp = np.argmax([raeward/wait if wait != 0 else reward for wait,reward in zip(wait_arr,reward_arr)])\n",
    "\n",
    "    action = indicies[tmp]\n",
    "    return action\n",
    "\n",
    "obs = env.reset()\n",
    "episode_return = 0\n",
    "episode_travel = 0\n",
    "return_arr = []\n",
    "travel_arr= []\n",
    "\n",
    "while True:\n",
    "    current_land = env.current_land\n",
    "    action = get_action(obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    episode_return += reward\n",
    "    next_land = env.current_land\n",
    "    travel_time = env.adjacency_matrix[current_land][next_land]\n",
    "    episode_travel += travel_time\n",
    "    #print(\"at\",env.current_time,\" the agent travels for \"+str(travel_time)+\" to go to ride \",env.current_location,\"in current land\",env.current_land,\"and get a reward of\",reward)\n",
    "    next_land = current_land\n",
    "\n",
    "\n",
    "    if (done):\n",
    "        return_arr += [episode_return]\n",
    "        travel_arr +=[episode_travel]\n",
    "        obs = env.reset()\n",
    "        episode_return = 0\n",
    "        episode_travel = 0\n",
    "        current_land = env.current_land\n",
    "\n",
    "    if len(return_arr) == 15:\n",
    "        break\n",
    "\n",
    "visualize_performance(return_arr,travel_arr,\"Deterministic Agent\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A random agent\n",
    "obs = env.reset()\n",
    "\n",
    "episode_return = 0\n",
    "episode_travel = 0\n",
    "return_arr = []\n",
    "travel_arr= []\n",
    "\n",
    "while True:\n",
    "    current_land = env.current_land\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    episode_return += reward\n",
    "    next_land = env.current_land\n",
    "    travel_time = env.adjacency_matrix[current_land][next_land]\n",
    "    episode_travel += travel_time\n",
    "    #print(\"at\",env.current_time,\" the agent travels for \"+str(travel_time)+\" to go to ride \",env.current_location,\"in current land\",env.current_land,\"and get a reward of\",reward)\n",
    "    next_land - current_land\n",
    "\n",
    "\n",
    "    if (done):\n",
    "        return_arr += [episode_return]\n",
    "        travel_arr +=[episode_travel]\n",
    "        obs = env.reset()\n",
    "        episode_return = 0\n",
    "        episode_travel = 0\n",
    "        current_land = env.current_land\n",
    "\n",
    "    if len(return_arr) == 15:\n",
    "        break\n",
    "\n",
    "visualize_performance(return_arr,travel_arr,\"Random Agent\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2C"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
