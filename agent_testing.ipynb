{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "忽略Keras RL的部分。Stable-baselines在下面  \n",
    "这个现在用的是gym 0.26.2, 在monitor.py里有一个return的问题可能需要更改return的value数量才能兼容  \n",
    "PPO感觉挺好的，在只120步的时候可以超过random选择的return。DQN在训练120步的时候不行。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras RL DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///gpfs/data/oermannlab/users/hz2212/DS-GA%203001-007/disneyenv\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hInstalling collected packages: disneyenv\n",
      "  Attempting uninstall: disneyenv\n",
      "    Found existing installation: disneyenv 0.0.1\n",
      "    Uninstalling disneyenv-0.0.1:\n",
      "      Successfully uninstalled disneyenv-0.0.1\n",
      "  Running setup.py develop for disneyenv\n",
      "Successfully installed disneyenv-0.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -e disneyenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/data/oermannlab/users/hz2212/DS-GA 3001-007/disneyenv/disneyenv/envs/disney.py:42: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  self.waittime = pd.read_csv(\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "action space does not inherit from `gymnasium.spaces.Space`, actual type: <class 'gym.spaces.discrete.Discrete'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdisneyenv\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdisneyenv/Disney-v0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/ds3001-007/lib/python3.10/site-packages/gymnasium/envs/registration.py:669\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, autoreset, apply_api_compatibility, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[38;5;66;03m# Run the environment checker as the lowest level wrapper\u001b[39;00m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m disable_env_checker \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m    667\u001b[0m     disable_env_checker \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m spec_\u001b[38;5;241m.\u001b[39mdisable_env_checker \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    668\u001b[0m ):\n\u001b[0;32m--> 669\u001b[0m     env \u001b[38;5;241m=\u001b[39m \u001b[43mPassiveEnvChecker\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;66;03m# Add the order enforcing wrapper\u001b[39;00m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec_\u001b[38;5;241m.\u001b[39morder_enforce:\n",
      "File \u001b[0;32m~/.conda/envs/ds3001-007/lib/python3.10/site-packages/gymnasium/wrappers/env_checker.py:23\u001b[0m, in \u001b[0;36mPassiveEnvChecker.__init__\u001b[0;34m(self, env)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(env)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\n\u001b[1;32m     21\u001b[0m     env, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction_space\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     22\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe environment must specify an action space. https://gymnasium.farama.org/content/environment_creation/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 23\u001b[0m \u001b[43mcheck_action_space\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_space\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\n\u001b[1;32m     25\u001b[0m     env, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobservation_space\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     26\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe environment must specify an observation space. https://gymnasium.farama.org/content/environment_creation/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     27\u001b[0m check_observation_space(env\u001b[38;5;241m.\u001b[39mobservation_space)\n",
      "File \u001b[0;32m~/.conda/envs/ds3001-007/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:74\u001b[0m, in \u001b[0;36mcheck_space\u001b[0;34m(space, space_type, check_box_space_fn)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m\"\"\"A passive check of the environment action space that should not affect the environment.\"\"\"\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(space, spaces\u001b[38;5;241m.\u001b[39mSpace):\n\u001b[0;32m---> 74\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspace_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m space does not inherit from `gymnasium.spaces.Space`, actual type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(space)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     76\u001b[0m     )\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(space, spaces\u001b[38;5;241m.\u001b[39mBox):\n\u001b[1;32m     79\u001b[0m     check_box_space_fn(space)\n",
      "\u001b[0;31mAssertionError\u001b[0m: action space does not inherit from `gymnasium.spaces.Space`, actual type: <class 'gym.spaces.discrete.Discrete'>"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import disneyenv\n",
    "\n",
    "env = gym.make('disneyenv/Disney-v0') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten,Input,Dropout,Reshape\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents import DQNAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape, output_shape):\n",
    "    model = Sequential([\n",
    "        Dense(512,activation=\"relu\",input_shape = (1,input_shape)),\n",
    "        Dropout(0.2),\n",
    "        Flatten(),\n",
    "        Dense(256,activation = \"relu\"),\n",
    "        Dropout(0.2),\n",
    "        Dense(128,activation = \"relu\"),\n",
    "        Dense(64,activation = \"relu\"),\n",
    "        Dense(output_shape,activation = \"linear\"),\n",
    "        Flatten()\n",
    "    ])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.2, nb_steps=10000)\n",
    "    memory = SequentialMemory(limit=2000,window_length = 1)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy,\n",
    "                  enable_dueling_network=True, dueling_type='avg',\n",
    "                  nb_actions=actions, nb_steps_warmup=10)\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 1, 512)            118784    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1, 512)            0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 106)               6890      \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 106)               0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 298,154\n",
      "Trainable params: 298,154\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(env.observation_space.n,env.action_space.n)\n",
    "dqn_agent = build_agent(model,env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_agent.compile(Adam(learning_rate=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 1000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\82751\\anaconda3\\envs\\my_environment\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gymnasium versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\82751\\anaconda3\\envs\\my_environment\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:187: UserWarning: \u001b[33mWARN: Future gymnasium versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A new day! Today is 2018-7-26\n",
      "Interval 1 (0 steps performed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\82751\\anaconda3\\envs\\my_environment\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:195: UserWarning: \u001b[33mWARN: The result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `<class 'numpy.ndarray'>`\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\82751\\anaconda3\\envs\\my_environment\\lib\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    1/10000 [..............................] - ETA: 1:21:38 - reward: 1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\82751\\anaconda3\\envs\\my_environment\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:133: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method should be an int or np.int64, actual type: <class 'numpy.ndarray'>\u001b[0m\n",
      "  logger.warn(f\"{pre} should be an int or np.int64, actual type: {type(obs)}\")\n",
      "c:\\Users\\82751\\anaconda3\\envs\\my_environment\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   11/10000 [..............................] - ETA: 46:40 - reward: -35.2727"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\82751\\anaconda3\\envs\\my_environment\\lib\\site-packages\\rl\\memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   72/10000 [..............................] - ETA: 55:26 - reward: -33.9792A new day! Today is 2017-7-24\n",
      "  119/10000 [..............................] - ETA: 54:14 - reward: -27.1176A new day! Today is 2017-10-23\n",
      "  191/10000 [..............................] - ETA: 53:18 - reward: -30.8901A new day! Today is 2018-4-12\n",
      "  253/10000 [..............................] - ETA: 52:33 - reward: -30.8043A new day! Today is 2018-6-22\n",
      "  307/10000 [..............................] - ETA: 52:05 - reward: -29.1824A new day! Today is 2018-6-20\n",
      "  358/10000 [>.............................] - ETA: 51:42 - reward: -29.3142A new day! Today is 2017-8-6\n",
      "  400/10000 [>.............................] - ETA: 51:25 - reward: -28.6212A new day! Today is 2018-6-28\n",
      "  449/10000 [>.............................] - ETA: 51:04 - reward: -28.3864A new day! Today is 2018-9-12\n",
      "  517/10000 [>.............................] - ETA: 50:21 - reward: -28.9052A new day! Today is 2018-8-21\n",
      "  564/10000 [>.............................] - ETA: 50:03 - reward: -28.6835A new day! Today is 2017-12-26\n",
      "  620/10000 [>.............................] - ETA: 49:50 - reward: -27.9808A new day! Today is 2018-1-4\n",
      "  657/10000 [>.............................] - ETA: 49:42 - reward: -27.4909A new day! Today is 2018-8-20\n",
      "  712/10000 [=>............................] - ETA: 52:03 - reward: -27.5822A new day! Today is 2017-11-24\n",
      "  764/10000 [=>............................] - ETA: 51:18 - reward: -27.3043A new day! Today is 2018-1-9\n",
      "  852/10000 [=>............................] - ETA: 50:37 - reward: -28.6092A new day! Today is 2018-8-7\n",
      "  906/10000 [=>............................] - ETA: 50:10 - reward: -28.6987A new day! Today is 2017-9-15\n",
      "  978/10000 [=>............................] - ETA: 49:39 - reward: -29.1131A new day! Today is 2018-9-14\n",
      " 1000/10000 [==>...........................] - ETA: 49:28 - reward: -29.2526done, took 330.622 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27c2d54e850>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn_agent.fit(env, nb_steps = 1000, visualize = False, verbose = 1) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent without RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///C:/Users/82751/OneDrive/Desktop/NYU/Reinforcement%20Learning/DS-GA-3001-007-Final-Project/disneyenv\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Installing collected packages: disneyenv\n",
      "  Attempting uninstall: disneyenv\n",
      "    Found existing installation: disneyenv 0.0.1\n",
      "    Uninstalling disneyenv-0.0.1:\n",
      "      Successfully uninstalled disneyenv-0.0.1\n",
      "  Running setup.py develop for disneyenv\n",
      "Successfully installed disneyenv-0.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -e disneyenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import disneyenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"disneyenv/Disney-v0\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "episode_return = []\n",
    "while True:\n",
    "    action = obs.action_space.sample\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    print(\"at\",env.current_time,\" the agent go to ride \",env.current_location,\"in current land\",env.current_land,\"and get a reward of\",reward)\n",
    "\n",
    "    episode_return += [reward]\n",
    "    if (done):\n",
    "        obs = env.reset()\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deterministic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A new day! Today is 2018-03-29\n",
      "at 2018-03-29 08:12:38.425107  the agent go to ride  11 in current land 1 and get a reward of 19.68595815464862\n",
      "at 2018-03-29 08:33:38.425107  the agent go to ride  12 in current land 1 and get a reward of 19.9\n",
      "at 2018-03-29 08:55:28.854042  the agent go to ride  38 in current land 4 and get a reward of 19.71595177523095\n",
      "at 2018-03-29 09:10:52.422618  the agent go to ride  65 in current land 8 and get a reward of 19.710719039255316\n",
      "at 2018-03-29 09:31:58.194264  the agent go to ride  83 in current land 10 and get a reward of 19.090380590370867\n",
      "at 2018-03-29 09:55:10.443845  the agent go to ride  48 in current land 5 and get a reward of 18.57958403133405\n",
      "at 2018-03-29 10:21:51.364237  the agent go to ride  90 in current land 12 and get a reward of 19.33179934618687\n",
      "at 2018-03-29 10:50:50.298196  the agent go to ride  4 in current land 0 and get a reward of 9.301776734439997\n",
      "at 2018-03-29 11:48:25.837328  the agent go to ride  23 in current land 2 and get a reward of 9.240768113545348\n",
      "at 2018-03-29 12:06:12.999430  the agent go to ride  50 in current land 6 and get a reward of 9.721396497383305\n",
      "at 2018-03-29 12:28:00.161532  the agent go to ride  22 in current land 2 and get a reward of 9.721396497383305\n",
      "at 2018-03-29 12:45:03.846043  the agent go to ride  77 in current land 10 and get a reward of 9.043859148961838\n",
      "at 2018-03-29 13:18:39.617689  the agent go to ride  63 in current land 8 and get a reward of 9.090380590370867\n",
      "at 2018-03-29 14:00:05.543733  the agent go to ride  1 in current land 0 and get a reward of 19.35678992641812\n",
      "at 2018-03-29 14:43:23.419794  the agent go to ride  15 in current land 1 and get a reward of 19.370206565558618\n",
      "at 2018-03-29 14:58:53.419794  the agent go to ride  10 in current land 1 and get a reward of 9.9\n",
      "at 2018-03-29 15:19:43.848729  the agent go to ride  29 in current land 4 and get a reward of 9.715951775230947\n",
      "at 2018-03-29 15:37:43.848729  the agent go to ride  32 in current land 4 and get a reward of 9.9\n",
      "at 2018-03-29 16:11:39.789677  the agent go to ride  100 in current land 15 and get a reward of 9.206765086142205\n",
      "at 2018-03-29 16:28:24.040482  the agent go to ride  52 in current land 6 and get a reward of 4.226248658798269\n",
      "at 2018-03-29 17:18:07.351084  the agent go to ride  13 in current land 1 and get a reward of 19.327815663443292\n",
      "at 2018-03-29 18:07:25.636516  the agent go to ride  89 in current land 12 and get a reward of 19.469524279183528\n",
      "at 2018-03-29 18:33:28.286596  the agent go to ride  30 in current land 4 and get a reward of 9.545583199826481\n",
      "at 2018-03-29 19:06:25.686020  the agent go to ride  78 in current land 10 and get a reward of 8.804334294094426\n",
      "at 2018-03-29 19:50:25.686020  the agent go to ride  81 in current land 10 and get a reward of 19.9\n",
      "at 2018-03-29 20:20:17.112327  the agent go to ride  69 in current land 9 and get a reward of 3.1642894886252355\n",
      "at 2018-03-29 21:00:08.538634  the agent go to ride  84 in current land 10 and get a reward of 18.164289488625236\n",
      "at 2018-03-29 21:53:12.223145  the agent go to ride  20 in current land 2 and get a reward of 19.043859148961836\n",
      "at 2018-03-29 22:23:23.384816  the agent go to ride  53 in current land 7 and get a reward of 4.6813972146526215\n",
      "at 2018-03-29 22:40:52.532365  the agent go to ride  86 in current land 11 and get a reward of 4.451420751587926\n",
      "The day is over! The reward is 409.40728257758695\n",
      "at 2018-03-29 23:03:25.630455  the agent go to ride  34 in current land 4 and get a reward of 9.044836517326937\n",
      "A new day! Today is 2018-01-18\n"
     ]
    }
   ],
   "source": [
    "# A deterministic agent\n",
    "obs = env.reset()\n",
    "reward_arr = env.ridesinfo.popularity.apply(lambda x:  5 if type(x)!=str else env.reward_dict[x]).to_numpy()\n",
    "\n",
    "def get_action(obs,reward_arr=reward_arr):\n",
    "    indicies = np.where((obs[\"operationStatus\"]  + ~obs[\"pastActions\"]) == 2)[0]\n",
    "    if len(indicies) == 0:\n",
    "        return 106\n",
    "    \n",
    "    wait_arr = obs[\"waitTime\"][indicies]\n",
    "    reward_arr = reward_arr[indicies]\n",
    "    tmp = np.argmax([reward/wait if wait != 0 else reward for wait,reward in zip(wait_arr,reward_arr)])\n",
    "\n",
    "    action = indicies[tmp]\n",
    "    return action\n",
    "\n",
    "\n",
    "episode_return = []\n",
    "while True:\n",
    "    action = get_action(obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    print(\"at\",env.current_time,\" the agent go to ride \",env.current_location,\"in current land\",env.current_land,\"and get a reward of\",reward)\n",
    "\n",
    "    episode_return += [reward]\n",
    "    if (done):\n",
    "        obs = env.reset()\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x27f1f67b5e0>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAGdCAYAAAA44ojeAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAABGCElEQVR4nO3de1iUdf7/8edwGhFhlOOAEKIiZqDlWbbNM+pmVrrZaUvT3a0sd1l1LevXd+27pWWb1Xetti3zWGvtph1WM6085JqJ5AEPKSoaKIgiMoA4wHD//qBmozyh4D3A63Fdc10x82F4ze2d8/LmnvdtMQzDQERERMSDeJkdQEREROTHVFBERETE46igiIiIiMdRQRERERGPo4IiIiIiHkcFRURERDyOCoqIiIh4HBUUERER8Tg+Zge4FFVVVRw9epTAwEAsFovZcUREROQiGIZBcXExUVFReHmd/xhJgywoR48eJSYmxuwYIiIicgmys7OJjo4+75oGWVACAwOB6hcYFBRkchoRERG5GA6Hg5iYGPf7+Pk0yILy/a91goKCVFBEREQamIs5PUMnyYqIiIjHUUERERERj6OCIiIiIh5HBUVEREQ8jgqKiIiIeBwVFBEREfE4KigiIiLicVRQRERExOOooIiIiIjHUUERERERj6OCIiIiIh5HBUVEREQ8jgqKiIiIuJ0sLWfGij3M3ZBlao4GeTVjERERqVuOMxW88UUWb27IosRZic3fl9Hdowls5mtKHhUUERGRJux0eSXzNx7itXUHKSqrACCxdRCTUxJoYTWvJqigiIiINEHOShdvf/UtL685wIkSJwDtw1sweXAHhibasVgspuZTQREREWlCKlxVvJeew/99lsnRojMAXBXcnNRB8dx8bWu8vcwtJt+7rJNkZ86cicViITU11X2fYRhMnz6dqKgo/P396devH7t27arxfU6nk4kTJxIaGkpAQAAjRowgJyfncqKIiIjIebiqDN7feoTBs9fx6NIMjhadwR7UjBm3JvHZ5L6M7BrtMeUELqOgpKWl8fe//53OnTvXuH/WrFnMnj2bOXPmkJaWht1uZ/DgwRQXF7vXpKamsmzZMpYsWcKGDRsoKSlh+PDhuFyuS38lIiIi8hOGYbByZx7DXlpP6jvbOFRwmpAAP54Y3om1f+zHXb2uwtfb8z7Ue0m/4ikpKeHuu+/m9ddf56mnnnLfbxgGL774Io8//jgjR44EYMGCBURERPD2229z//33U1RUxNy5c1m0aBGDBg0CYPHixcTExPDpp58yZMiQOnhZIiIiTZthGKzbd5znV+0j40gRAEHNfLi/bzvGJrchwMQTYC/GJVWmhx56iBtvvNFdML6XlZVFXl4eKSkp7vusVit9+/Zl48aNAKSnp1NRUVFjTVRUFImJie41IiIiUnsVrirSDxfy8pr9jHx1I2PnpZFxpIgAP28mDmjPF48M4KH+7T2+nMAlHEFZsmQJX3/9NWlpaT95LC8vD4CIiIga90dERHD48GH3Gj8/P1q1avWTNd9//485nU6cTqf7a4fDUdvYIiIijY6rymDX0SK+PFDAlwcLSMs6SWn5f0+X8PPx4t7esTzYrx0hLawmJq29WhWU7Oxsfv/737Nq1SqaNWt2znU//miSYRgX/LjS+dbMnDmTJ598sjZRRUREGp2qKoO9x4rZeKCALw8U8FVWAcVnKmusadncl95xISS3D2HINXYigs79fu3JalVQ0tPTyc/Pp1u3bu77XC4X69evZ86cOezduxeoPkoSGRnpXpOfn+8+qmK32ykvL6ewsLDGUZT8/HySk5PP+nOnTZvGpEmT3F87HA5iYmJqE11ERKTBMQyDA8dL+PJAARsPFLDpYAGFpytqrAm0+tCrbTC924aQ3C6UjvZAvDzo0ziXqlYFZeDAgWRkZNS477777qNjx4488sgjtG3bFrvdzurVq7nuuusAKC8vZ926dTz77LMAdOvWDV9fX1avXs3o0aMByM3NZefOncyaNeusP9dqtWK1NqxDUyIiIpfimOMMGzJPsGH/Cf6z/wT5xc4ajzf386ZHm2D6tAuhT9sQrokKwscDP4VzuWpVUAIDA0lMTKxxX0BAACEhIe77U1NTmTFjBvHx8cTHxzNjxgyaN2/OXXfdBYDNZmP8+PFMnjyZkJAQgoODmTJlCklJST856VZERKSxK3VW8lVWAV9knmBD5gky80tqPG718aJbbCv6tK3+tU3n6JYe+bHgulbnp/FOnTqVsrIyJkyYQGFhIb169WLVqlUEBga617zwwgv4+PgwevRoysrKGDhwIPPnz8fb27uu44iIiHiUSlcVO44UuY+SbP22kAqX4X7cYoGk1jaubx/K9e1D6Rrbima+Te/90WIYhnHhZZ7F4XBgs9koKioiKCjI7DgiIiLnZBgGhwpOsyHzOBv2n2DjgZ+e2BoT7M/17cO4vn0oye1CaBXgZ1La+lWb92/P/yC0iIhIA/VNnoOH3vqaA8dLa9wf1MyHn7UP5WftQ/l5fCixIQEmJfRcKigiIiL1IPvkae6du5n8Yie+3ha6xbaq/rVNfBhJrW0edd0bT6SCIiIiUseOFzu5Z+5X5Bc7SYgIZMlvezfaX9vUl8Z/GrCIiMgVVHymgrHzNnOo4DTRrfxZOL6nysklUEERERGpI2cqXPxm4RZ2HXUQEuDHovG9GuwkV7OpoIiIiNQBV5XB75dsZdPBk7Sw+rBgXE/iQnXy66VSQREREblMhmHw/97P4JNdx/Dz9uLv93YjsbXN7FgNmgqKiIjIZfrLqr38Y3M2Xhb4vzuvJbldqNmRGjwVFBERkcswd0MWL685AMDTtyYxNDHyAt8hF0MFRURE5BK9v/UIf/73bgD+OCSBO3teZXKixkMFRURE5BKs2ZvPlH9uB+C+n7VhQr92JidqXFRQREREain9cCEPLk6nssrglmujeOLGTlgsmgxbl1RQREREamHfsWLGzU/jTEUV/RLCeO62LnhpbH2dU0ERERG5SDmF1dfXKSqr4LqrWvLK3V3x9dZbaX3QVhUREbkIBSVO7p27mTzHGeLDWzBvbA+a++mSdvVFBUVEROQCSpyV3Dc/jYMnSmndsvr6Oi2b6/o69UkFRURE5DyclS4eWJTOjpwiggP8WDi+J5E2f7NjNXoqKCIiIudQ4api4ttb2bD/BM39vJk3tgftwlqYHatJUEERERE5C1eVwaR3t7Nq9zH8fLx4/d7udIlpaXasJkMFRURE5EeqqgwefW8HH20/iq+3hb/9qis/a6/r61xJKigiIiI/YBgG0z/axT/Tc6ov/nfHdQzoGGF2rCZHBUVEROQ7hmHwzMffsPDLw1gs8PzoLgxL0sX/zKCCIiIi8p2XPsvktfUHAXj6liRuvS7a5ERNlwqKiIgI8Ld1B3jx00wA/md4J+7qpSsTm0kFRUREmrwFGw/xzMffAPDHIQmMuz7O5ESigiIiIk3au2nZ/OnDXQBMHNCeh/q3NzmRgAqKiIg0YR9sO8IjS3cA8Ovr45g0uIPJieR7KigiItIkrdyZx6R3t2MYcHevq3j8xquxWCxmx5LvqKCIiEiTs2ZvPhP/8TWuKoNRXaP5882JKiceRgVFRESalI0HTvDAonQqXAY3do7k2VFJeHmpnHgaFRQREWky0g+f5NcLtuCsrGLQ1RG8ePu1+HjrrdAT6U9FRESahIycIsa+mcbpchc/jw9lzl3X4aty4rF8zA4gIiJSH06WlvNNroPduQ6+yStm9e5jFDsr6RkXzN/v6U4zX2+zI8p51Ko6vvrqq3Tu3JmgoCCCgoLo06cPH3/8sfvxsWPHYrFYatx69+5d4zmcTicTJ04kNDSUgIAARowYQU5OTt28GhERaXIqXFXszSvmg21HmPnxHsa8uZmeT39K1z+v5q43vuKp5Xv4V3oORWUVXBvTkjfH9sDfT+XE09XqCEp0dDTPPPMM7dtXD7FZsGABN998M1u3buWaa64BYOjQocybN8/9PX5+fjWeIzU1lY8++oglS5YQEhLC5MmTGT58OOnp6Xh7a4cREZFzKywtZ3eugz25DvbkFrMn18H+/BLKXVVnXR8b0pyr7UF0jAykU2QQfRPCsProvaYhsBiGYVzOEwQHB/Pcc88xfvx4xo4dy6lTp3j//ffPuraoqIiwsDAWLVrE7bffDsDRo0eJiYlhxYoVDBky5KJ+psPhwGazUVRURFBQ0OXEFxGRBmLp1zk8+l7GWctIC6sPHe2BdIwM5OrIIDrag+hoDyTAqjMZPElt3r8v+U/O5XLxz3/+k9LSUvr06eO+f+3atYSHh9OyZUv69u3L008/TXh4OADp6elUVFSQkpLiXh8VFUViYiIbN248Z0FxOp04nc4aL1BERJqOLzKPM/VfO6isMogJ9ueaSFt1EfnuyEjrlv76qHAjU+uCkpGRQZ8+fThz5gwtWrRg2bJldOrUCYBhw4Zx2223ERsbS1ZWFk888QQDBgwgPT0dq9VKXl4efn5+tGrVqsZzRkREkJeXd86fOXPmTJ588snaRhURkUZgT66DBxd/TWWVwYguUbx4+7UqI01ArQtKQkIC27Zt49SpU7z33nuMGTOGdevW0alTJ/evbQASExPp3r07sbGxLF++nJEjR57zOQ3DOO8Ev2nTpjFp0iT31w6Hg5iYmNpGFxGRBia3qIz75qVR4qykV1wwz93WWeWkiah1QfHz83OfJNu9e3fS0tJ46aWXeO21136yNjIyktjYWDIzMwGw2+2Ul5dTWFhY4yhKfn4+ycnJ5/yZVqsVq9Va26giItKAOc5UcN+8NPIcZ2gf3oK/39NdJ7g2IZc9ocYwjBrnh/xQQUEB2dnZREZGAtCtWzd8fX1ZvXq1e01ubi47d+48b0EREZGmpbyyigmLv+abvGLCAq3Mv68Htua+ZseSK6hWR1Aee+wxhg0bRkxMDMXFxSxZsoS1a9eycuVKSkpKmD59OqNGjSIyMpJDhw7x2GOPERoayq233gqAzWZj/PjxTJ48mZCQEIKDg5kyZQpJSUkMGjSoXl6giIg0LIZhMG1pBhv2n6C5nzdvjulBdKvmZseSK6xWBeXYsWPcc8895ObmYrPZ6Ny5MytXrmTw4MGUlZWRkZHBwoULOXXqFJGRkfTv35933nmHwMBA93O88MIL+Pj4MHr0aMrKyhg4cCDz58/XDBQREQHgxU8zee/rHLws8PJdXUmKtpkdSUxw2XNQzKA5KCIijdO7adlMfW8HADNuTeKuXleZnEjqUm3ev3WVJBER8Qjr9h1n2rIMAB7q307lpIlTQREREdPtOlrEhMXpuKoMbr42iikpCWZHEpOpoIiIiKmOnipj3Pw0Sstd9G4bzKxfdj7vbCxpGlRQRETENEVlFYydt5ljDifx4S14TbNO5DsqKCIiYoryyioeXJzOvmMlhAdamT+uJzZ/zTqRaiooIiJyxRmGwaPv7WDjgYLqWSdje9C6pb/ZscSDqKCIiMgVN3v1PpZuPYK3l4WX7+5KYmvNOpGaVFBEROSKWrL5W/76+X4Anr4lkf4J4SYnEk9U64sFioiIXIqi0xW8seEgr6w9AMDEAe25o6dmncjZqaCIiEi9Kiqr4M0NWby5IYtiZyUAo7pGM2lwB5OTiSdTQRERkXpRfKaCef85xBtfHMRxprqYJEQEkjooniHX2DXrRM5LBUVEROpUibOS+f/J4vUvsigqqwAgPrwFqYM6MCzRjpeXiolcmAqKiIjUiVJnJQu+PMTr6w9SeLq6mLQLC+D3gzpwY1Ik3iomUgsqKCIicllOl1ey8MvD/H39QU6WlgPQNjSA3w+KZ3jnKBUTuSQqKCIicknKyl0s3nSY19Yf4ERJdTFpE9Kc3w2MZ0SXKHy8NclCLp0KioiI1MqZChdvffUtr649wIkSJwBXBVcXk1uuVTGRuqGCIiIiF6WqyuCD7Uf4yyf7OHKqDIDoVv78bkA8t3Ztja+KidQhFRQREbmgDZknmPnxHnYddQAQaWvG7wbG88tu0SomUi9UUERE5Jx2H3XwzMpvWL/vOACBVh8e7N+OcT+Lo5mvt8nppDFTQRERkZ84eqqM51ftY+nWHAwDfL0t/Kp3LBMHxBMc4Gd2PGkCVFBERMTNcaaCV9ce4M0NWTgrqwC4sXMkU4ckEBsSYHI6aUpUUEREhPLKKhZvOsxfP890D1nr2SaYab/oyHVXtTI5nTRFKigiIk2YYRgsz8hl1sq9fHvyNADtw1vw6NCODLw6XNfLEdOooIiINFFfHSxgxoo9bM8pAiAs0MofBnVgdPdozTIR06mgiIg0QS+v2c9zn+wFoLmfN/ff0I5f/zyOAKveFsQzaE8UEWli3k3LdpeTO3vG8IfBHQgPbGZyKpGaVFBERJqQz785xrRlGQA82K8djwztaHIikbPTLxlFRJqIrd8WMuGtr3FVGYzs2pqpQxLMjiRyTiooIiJNwMHjJYybn8aZiir6dgjj2VGd9Qkd8WgqKCIijVx+8RnufXMzhacr6Bxt45W7u+r6OeLxtIeKiDRixWcquG9eGjmFZcSGNOfNsT30SR1pEFRQREQaqfLKKh5YnM6uow5CAvxYOK4noS2sZscSuSi1KiivvvoqnTt3JigoiKCgIPr06cPHH3/sftwwDKZPn05UVBT+/v7069ePXbt21XgOp9PJxIkTCQ0NJSAggBEjRpCTk1M3r0ZERACoqjL447+285/9BTT382befT10LR1pUGpVUKKjo3nmmWfYsmULW7ZsYcCAAdx8883uEjJr1ixmz57NnDlzSEtLw263M3jwYIqLi93PkZqayrJly1iyZAkbNmygpKSE4cOH43K56vaViYg0Yc+s/IYPth3Fx8vCq7/qRufolmZHEqkVi2EYxuU8QXBwMM899xzjxo0jKiqK1NRUHnnkEaD6aElERATPPvss999/P0VFRYSFhbFo0SJuv/12AI4ePUpMTAwrVqxgyJAhF/UzHQ4HNpuNoqIigoKCLie+iEij88YXB3lq+R4Anr+tC6O6RZucSKRabd6/L/kcFJfLxZIlSygtLaVPnz5kZWWRl5dHSkqKe43VaqVv375s3LgRgPT0dCoqKmqsiYqKIjEx0b3mbJxOJw6Ho8ZNRER+6sPtR93lZOrQBJUTabBqXVAyMjJo0aIFVquVBx54gGXLltGpUyfy8vIAiIiIqLE+IiLC/VheXh5+fn60atXqnGvOZubMmdhsNvctJiamtrFFRBq9jftPMPndbQCMTW7Dg33bmRtI5DLUuqAkJCSwbds2Nm3axIMPPsiYMWPYvXu3+/EfD/4xDOOCw4AutGbatGkUFRW5b9nZ2bWNLSLSqO0+6uC3i9KpcBn8IsnOE8M7aRCbNGi1Lih+fn60b9+e7t27M3PmTLp06cJLL72E3W4H+MmRkPz8fPdRFbvdTnl5OYWFhedcczZWq9X9yaHvbyIiUi375GnGzttMibOSXnHBzB59Ld5eKifSsF32HBTDMHA6ncTFxWG321m9erX7sfLyctatW0dycjIA3bp1w9fXt8aa3Nxcdu7c6V4jIiIXr7C0nDHzNpNf7CQhIpC/39udZr7eZscSuWy1Gif42GOPMWzYMGJiYiguLmbJkiWsXbuWlStXYrFYSE1NZcaMGcTHxxMfH8+MGTNo3rw5d911FwA2m43x48czefJkQkJCCA4OZsqUKSQlJTFo0KB6eYEiIo1VWbmLcQvSOHi8lEhbM+aP64HN39fsWCJ1olYF5dixY9xzzz3k5uZis9no3LkzK1euZPDgwQBMnTqVsrIyJkyYQGFhIb169WLVqlUEBga6n+OFF17Ax8eH0aNHU1ZWxsCBA5k/fz7e3mr8IiIXq9JVxcR/fM3Wb09h8/dl4bieRNr8zY4lUmcuew6KGTQHRUSaMsMweOS9Hby7JQerjxeLf92LHm2CzY4lckFXZA6KiIiY4/lV+3h3Sw5eFvjrndepnEijpIIiItKALNh4iDlr9gPw9K1JpFxjNzmRSP1QQRERaSCW78hl+kfV1z77w6AO3NnzKpMTidQfFRQRkQZg44ET/OGdbRgG3N3rKn43sL3ZkUTqlQqKiIiH233Uwf0L0yl3VTH0Gjv/e3OipsRKo6eCIiLiwbJPnmbMvM0UOyvp2SaYF+/QlFhpGlRQREQ8VEGJkzFvbub4d1NiXx+jKbHSdKigiIh4oNPllYxbsIWDJ0pp3dKfBeN6akqsNCkqKCIiHqbCVcWEt75me/YpWjb3ZcG4nthtzcyOJXJFqaCIiHiQ76fErt17nGa+Xrw5tgftw1uYHUvkilNBERHxIM+s/IalXx/B28vCK3d3petVrcyOJGIKFRQREQ8xd0MWr607CMDMkUkM6BhhciIR86igiIh4gA+2HeHP/94NwB+HJDC6e4zJiUTMpYIiImKyLzKPM+Wf2wEYm9yGCf3amZxIxHw+ZgcQEWmKKlxV7DxSxFdZJ/nrZ5lUuAxu7BzJ/wzvpCmxIqigiIhcEWcqXGzLPsXmrJNszjpJ+uFCyipc7seT24Uwe3QXvDQlVgRQQRERqRclzkq+PlzoLiTbsk9R7qqqsaZlc196tAmmT9sQ7ugZg9VHU2JFvqeCIiJSB06dLiftUCGbswrYnHWSnUcduKqMGmvCAq30igumV1wwPeNCiA9voSMmIueggiIichkOHi/h+dX7WJGRi1GzjxAT7E/PNiHfFZJgYkOa6/wSkYukgiIicglyi8p46dNM/pme4z5S0i4sgJ5xIfRuG0yPNsFEtfQ3OaVIw6WCIiJSCydLy3l17X4WfHmY8srqc0oGdgxnypAEro4MMjmdSOOhgiIichFKnJXM/SKL1784SImzEoCeccFMHZJA9zbBJqcTaXxUUEREzsNZ6eKtTd/y8pr9FJSWA9ApMoipQxPo2yFM55SI1BMVFBGRs6h0VbF06xFe+jSTI6fKAIgLDWDS4A7cmBSpT9+I1DMVFBGRHzAMg5U78/jLqr0cOF4KgD2oGb8fFM8vu0Xj660rhIhcCSooIiLf2ZB5glmffMOOnCKgepDahH7tuLdPG5r5aoiayJWkgiIiTd6+Y8X8+d+7+SLzBADN/bz59fVx/PqGtgQ18zU5nUjTpIIiIk1WQYmTFz7dx9tffUuVAb7eFu7uFcvDA9oT2sJqdjyRJk0FRUSaHGeliwUbD/HXz/dTfKb6I8NDrolg2rCraRMaYHI6EQEVFBFpQgzDYNXuY8xYsYfDBacBuCYqiP93Yyf6tAsxOZ2I/JAKiog0CbuOFvHnf+9m08GTQPWF+/44JIFRXaPx1keGRTyOCoqINGr5xWd4/pN9vJuejWGA1ceL3/y8LQ/0a0cLq/4KFPFUtfpA/8yZM+nRoweBgYGEh4dzyy23sHfv3hprxo4di8ViqXHr3bt3jTVOp5OJEycSGhpKQEAAI0aMICcn5/JfjYjId85UuHh5zX76P7eWd7ZUl5ObukTx2eS+TBmSoHIi4uFqVVDWrVvHQw89xKZNm1i9ejWVlZWkpKRQWlpaY93QoUPJzc1131asWFHj8dTUVJYtW8aSJUvYsGEDJSUlDB8+HJfLdfmvSESaNMMw+Gj7UQY+v47nPtlLabmLa2Na8t6Dyfz1zuuIbtXc7IgichFq9U+IlStX1vh63rx5hIeHk56ezg033OC+32q1Yrfbz/ocRUVFzJ07l0WLFjFo0CAAFi9eTExMDJ9++ilDhgyp7WsQEQFgW/Ypnvr3brYcLgQg0taMR4Z2ZESXKI2mF2lgLusYZ1FR9bTF4OCaV/Jcu3Yt4eHhtGzZkr59+/L0008THh4OQHp6OhUVFaSkpLjXR0VFkZiYyMaNG89aUJxOJ06n0/21w+G4nNgi0shknzzNc5/s5cPtRwHw9/XmwX7t+M3P2+LvpwmwIg3RJRcUwzCYNGkS119/PYmJie77hw0bxm233UZsbCxZWVk88cQTDBgwgPT0dKxWK3l5efj5+dGqVasazxcREUFeXt5Zf9bMmTN58sknLzWqiDRSjjMVvLxmP/P+c4jyyiosFhjVNZopKQnYbc3Mjicil+GSC8rDDz/Mjh072LBhQ437b7/9dvd/JyYm0r17d2JjY1m+fDkjR4485/MZhnHOy5ZPmzaNSZMmub92OBzExMRcanQRaeAqXFW8/dW3vPRZJidLywFIbhfC4zdezTVRNpPTiUhduKSCMnHiRD788EPWr19PdHT0eddGRkYSGxtLZmYmAHa7nfLycgoLC2scRcnPzyc5Ofmsz2G1WrFaNXZapKkzDINP9+Qz8+M9HPzuSsPtw1vw2C860j8h/Jz/yBGRhqdWBcUwDCZOnMiyZctYu3YtcXFxF/yegoICsrOziYyMBKBbt274+vqyevVqRo8eDUBubi47d+5k1qxZl/ASRKQpyMgp4ukV/x20FhLgR+rgDtzZIwYf71p9IFFEGoBaFZSHHnqIt99+mw8++IDAwED3OSM2mw1/f39KSkqYPn06o0aNIjIykkOHDvHYY48RGhrKrbfe6l47fvx4Jk+eTEhICMHBwUyZMoWkpCT3p3pERL539FQZf/lkL0u3HgGqB62Nvz6OB/u1I1BXGhZptGpVUF599VUA+vXrV+P+efPmMXbsWLy9vcnIyGDhwoWcOnWKyMhI+vfvzzvvvENgYKB7/QsvvICPjw+jR4+mrKyMgQMHMn/+fLy9dba9iFQrcVby6tr9vPFFFs7KKgBuva41U4Yk0Lqlv8npRKS+WQzDMMwOUVsOhwObzUZRURFBQUFmxxGROlTpqmJJWjYvfrqPEyXVJ8D2jAvm/914NZ2jW5obTkQuS23evzXrWUQ8xrp9x3nq37vJzC8BIC40gEeHdSSlU4ROgBVpYlRQRMR0+/OLeXr5HtbsPQ5Ay+a+pA6M5+7esfjqBFiRJkkFRURMU1hazouf7mPxV9/iqjLw8bIwJrkNvxsQj625ToAVacpUUETkiiuvrGLRpsO89Ok+HGcqARh0dQSP/aIjbcNamJxORDyBCoqIXDGGYfDZnnyeXrGHrBPVg9Y62gN5YngnftY+1OR0IuJJVFBE5IrYk+vgqeW7+c/+AgBCW/gxOSWB0d1j8NaVhkXkR1RQRKRenShx8vyqfbyT9i1VBvh5ezHu+jge6q9BayJybiooIlIvnJUu5v3nEHM+30+Js/o8kxuTInl0WEdigpubnE5EPJ0KiojUuS8PFDD1ve1knywDIKm1jSeGd6JnXLDJyUSkoVBBEZE6tfXbQsbNT6OswkVEkJWpQzpy63Wt8dJ5JiJSCyooIlJnDh4vcZeTGzqE8bdfdaW5n/6aEZHa04hGEakT+Y4z3PvmZgpPV9A52sard6uciMilU0ERkcvmOFPBmHlp5BSW0SakOW+O7UGAVeVERC6dCoqIXBZnpYv7F6azJ9dBaAsrC8f1IrSF1exYItLAqaCIyCWrqjKY9O52vjxYQAurD/Pv68FVIfoIsYhcPhUUEbkkhmHwv//ezfIdufh6W/jbr7qR2NpmdiwRaSRUUETkkry67gDzNx4C4PnR13J9vK6lIyJ1RwVFRGrtn1uymbVyLwBPDO/EiC5RJicSkcZGBUVEamXNN/k8ujQDgPtvaMv46+NMTiQijZEKiohctK3fFjLhra9xVRmMvK41jwztaHYkEWmkVFBE5KL8eErss7/srPH1IlJvVFBE5ILONiXW11t/fYhI/dHfMCJyXpoSKyJmUEERkXNyVrr47cItmhIrIlecCoqInFVVlcGkd7az6eBJTYkVkStOBUVEfsI9JTZDU2JFxBwqKCLyEz+cEvuX27poSqyIXHEqKCJSw4+nxN58bWuTE4lIU6SCIiJumhIrIp5CBUVEAE2JFRHPooIiIhzQlFgR8TAqKCJN3DHHGe6dqymxIuJZavW30MyZM+nRoweBgYGEh4dzyy23sHfv3hprDMNg+vTpREVF4e/vT79+/di1a1eNNU6nk4kTJxIaGkpAQAAjRowgJyfn8l+NiNSK40wFY97czJFTmhIrIp6lVgVl3bp1PPTQQ2zatInVq1dTWVlJSkoKpaWl7jWzZs1i9uzZzJkzh7S0NOx2O4MHD6a4uNi9JjU1lWXLlrFkyRI2bNhASUkJw4cPx+Vy1d0rE5HzOlNRPSX2m7xiTYkVEY9jMQzDuNRvPn78OOHh4axbt44bbrgBwzCIiooiNTWVRx55BKg+WhIREcGzzz7L/fffT1FREWFhYSxatIjbb78dgKNHjxITE8OKFSsYMmTIBX+uw+HAZrNRVFREUFDQpcYXabJcVQYT//E1KzLyaGH1Yclve2sQm4jUu9q8f1/WL5qLiooACA4OBiArK4u8vDxSUlLca6xWK3379mXjxo0ApKenU1FRUWNNVFQUiYmJ7jU/5nQ6cTgcNW4icmkMw+B/P9rFiow8fL0tvHaPpsSKiOe55IJiGAaTJk3i+uuvJzExEYC8vDwAIiIiaqyNiIhwP5aXl4efnx+tWrU655ofmzlzJjabzX2LiYm51NgiTd4raw+w4MvDADw/+lp+1l5TYkXE81xyQXn44YfZsWMH//jHP37ymMVS8+OJhmH85L4fO9+aadOmUVRU5L5lZ2dfamyRJu3dLdk890n1ie3/M7wTI7pEmZxIROTsLqmgTJw4kQ8//JA1a9YQHR3tvt9utwP85EhIfn6++6iK3W6nvLycwsLCc675MavVSlBQUI2biNTO598cY9p3U2If6NuOcZoSKyIerFYFxTAMHn74YZYuXcrnn39OXFzNv+Di4uKw2+2sXr3afV95eTnr1q0jOTkZgG7duuHr61tjTW5uLjt37nSvEZG69fUPp8R2bc0jQxPMjiQicl61Gnjw0EMP8fbbb/PBBx8QGBjoPlJis9nw9/fHYrGQmprKjBkziI+PJz4+nhkzZtC8eXPuuusu99rx48czefJkQkJCCA4OZsqUKSQlJTFo0KC6f4UiTdz+/OopsWcqqujbIYxnR3W+4K9cRUTMVquC8uqrrwLQr1+/GvfPmzePsWPHAjB16lTKysqYMGEChYWF9OrVi1WrVhEYGOhe/8ILL+Dj48Po0aMpKytj4MCBzJ8/H29v78t7NSJSwzHHGca8uZlTpyvoEm3jFU2JFZEG4rLmoJhFc1BEzq+s3MWCLw/xt3UHOHW6grjQAP71QB9CNIhNRExUm/dvzbQWaUTKK6tYkvYtf/18P8eLnQB0iGjB3DE9VE5EpEFRQRFpBCpdVSzbeoQXP83kyKkyAGKC/Ukd2IFbrmuNt65MLCINjAqKSANWVWXw8c48nl+9l4PHq6+JFR5oZeLAeG7vHoOfj843EZGGSQVFpAEyDIO1e4/z3Cd72Z1bfemHls19mdCvHff0boO/n044F5GGTQVFpIHZdLCA5z7ZS/rh6mGHLaw+/PrncYy/Po7AZr4mpxMRqRsqKCINxPbsU/xl1V6+yDwBgNXHi7HJbXigbztaBfiZnE5EpG6poIh4uH3Hinl+1V4+2XUMAF9vC3f0uIqHB7QnIqiZyelEROqHCoqIhzp0opQXPt3Hh9uPYhjgZYFbr4smdVA8McHNzY4nIlKvVFBEPMyRU2X89bNM/pmeg6uqeo7isEQ7k1M60D488ALfLSLSOKigiHiI48VOXl6zn7e/+pZyVxUA/RPCmJySQGJrm8npRESuLBUUEZOdOl3O39YdZMHGQ5RVuADo0zaEKUM60C022OR0IiLmUEERMUnxmQre3HCIN744SLGzEoBrY1ryxyEJ/Kx9qMnpRETMpYIicoWVlbtYtOkQr649QOHpCgCujgxi8uAODLw6HItFY+lFRFRQRK6Q7y/kN+fz/eR/dyG/tmEBTBrcgV8kRuKl6+WIiLipoIjUs6oqg2VbjzB79T73hfyiW/nz+4Hx3Hpda3y8db0cEZEfU0ERqUcb95/gqeV73NfLiQiy8vAAXchPRORCVFBE6sH+/GJmrviGz77JByCwmQ8P9W/P2OQ2NPPVhfxERC5EBUWkDp0ocfLip/v4x+ZsXFUGPl4WftU7lt8NjCdY18sREbloKigideBMhYs3/5PFK2sOUPLdR4YHd4pg2rCOtA1rYXI6EZGGRwVF5DJUVRl8uP0oz32y130CbFJrG4/feDW924aYnE5EpOFSQRG5RF8dLODpFXvYkVMEQJStGX8cmsDNXVrrI8MiIpdJBUWklg4eL+GZj79h1e5jALSw+vBgv3aMvz5OJ8CKiNQRFRSRi1RYWs5Ln2WyeNNhKqsMvCxwZ8+rSB3UgbBAq9nxREQaFRUUkQtwVrpYsPEQf/18P8Vnqk+A7Z8QxmO/uJr4iECT04mINE4qKCLnYBgG/96Ry7MrvyGnsPoE2I72QP7fjZ24Pl4X8xMRqU8qKCJnkX74JE8t38PWb08BEB5oZcqQBEZ1jcZbJ8CKiNQ7FRSRHzhcUMqslXtZnpELgL+vNw/0bcdvboijuZ/+dxERuVL0N64IUHS6gr9+nsmCLw9R4TKwWGB0txgmp3QgPKiZ2fFERJocFRRp0sorq1i86TD/93kmp05XAPDz+FAe+8XVXB0ZZHI6EZGmSwVFmiTDMPhkVx7PfPwNhwpOA9AhogWP/eJq+nYIw2LReSYiImZSQZEmZ3v2KZ5evofNh04CENrCj0mDExjdPRofby+T04mICKigSBNy6EQpz6/ex0fbjwJg9fHiNz9vywP92tHCqv8VREQ8Sa3/ubh+/XpuuukmoqKisFgsvP/++zUeHzt2LBaLpcatd+/eNdY4nU4mTpxIaGgoAQEBjBgxgpycnMt6ISLncsxxhseXZTBo9jp3ORnZtTVr/9iPKUMSVE5ERDxQrf9mLi0tpUuXLtx3332MGjXqrGuGDh3KvHnz3F/7+fnVeDw1NZWPPvqIJUuWEBISwuTJkxk+fDjp6el4e+taJlI3ik5X8Lf1B5j3nyzOVFQB0C8hjCkpCSS2tpmcTkREzqfWBWXYsGEMGzbsvGusVit2u/2sjxUVFTF37lwWLVrEoEGDAFi8eDExMTF8+umnDBkypLaRRGooK3cxb2MWf1t7AMd3o+m7xbZi6pAEerUNMTmdiIhcjHo5tr127VrCw8Np2bIlffv25emnnyY8PByA9PR0KioqSElJca+PiooiMTGRjRs3nrWgOJ1OnE6n+2uHw1EfsaWBq3BV8U5aNv/3WSb5xdX7S0JEIH8cksDAq8P1yRwRkQakzgvKsGHDuO2224iNjSUrK4snnniCAQMGkJ6ejtVqJS8vDz8/P1q1alXj+yIiIsjLyzvrc86cOZMnn3yyrqNKI1FVZfDRjqPMXr2Pw999ZDi6lT+TBnfg5mtbazS9iEgDVOcF5fbbb3f/d2JiIt27dyc2Npbly5czcuTIc36fYRjn/BfutGnTmDRpkvtrh8NBTExM3YWWBskwDNbuO86slXvZk1t9VC20hR8TB8RzR88YrD46n0lEpKGq948vREZGEhsbS2ZmJgB2u53y8nIKCwtrHEXJz88nOTn5rM9htVqxWq31HVUakC2HTjJr5V73LJNAqw+/vaEt466PI0CfyhERafDq/W/ygoICsrOziYyMBKBbt274+vqyevVqRo8eDUBubi47d+5k1qxZ9R1HGri9ecU898k3fLonHwA/Hy/GJrfhwb7taBXgd4HvFhGRhqLWBaWkpIT9+/e7v87KymLbtm0EBwcTHBzM9OnTGTVqFJGRkRw6dIjHHnuM0NBQbr31VgBsNhvjx49n8uTJhISEEBwczJQpU0hKSnJ/qkfkx3IKTzN79T6WbT2CYYC3l4XR3aP53cB4Im3+ZscTEZE6VuuCsmXLFvr37+/++vtzQ8aMGcOrr75KRkYGCxcu5NSpU0RGRtK/f3/eeecdAgMD3d/zwgsv4OPjw+jRoykrK2PgwIHMnz9fM1DkJwpKnMxZs5+3Nn1Luat6lskvkuxMTkmgXVgLk9OJiEh9sRiGYZgdorYcDgc2m42ioiKCgnTF2caoxFnJG18c5PX1ByktdwGQ3C6ER4Z2pEtMS3PDiYjIJanN+7fOJhSP4qx08fZX3zLn8/0UlJYDkNg6iEeGduTn8WEmpxMRkStFBUU8QlWVwQfbj/D8qn3kFJYB0CakOVOGJPCLxEi8NMtERKRJUUERUxmGwZq9+cxauZdv8ooBCA+08vtB8YzuHoOvd62vZykiIo2ACoqYJv3wSZ79+AezTJr58GC/dtyXHIe/n06YFhFpylRQ5IqrdFUx9V87WLr1CADW72eZ9GtHy+aaZSIiIioocoUZhsFjyzJYuvUIXhYY3T2G3w/SLBMREalJBUWuqOdX7ePdLTl4WeBvv+pGyjV2syOJiIgH0hmIcsUs2HiIOWuqpxDPuDVJ5URERM5JBUWuiOU7cpn+0S4AJg3uwB09rzI5kYiIeDIVFKl3Gw+c4A/vbMMw4Fe9r2LigPZmRxIREQ+ngiL1avdRB/cvTKfcVcXQa+w8OSIRi0VD10RE5PxUUKTeZJ88zZh5myl2VtIzLpgX77gWb02EFRGRi6CCIvWioMTJvW9u5nixk472QF6/tzvNfDV8TURELo4KitS5Umcl4+ankXWilNYt/Vkwric2f1+zY4mISAOigiJ1qsJVxYNvfc32nCJaNfdlwbieRAQ1MzuWiIg0MCooUmeqqgwe+dcO1u87TjNfL+aO7UH78BZmxxIRkQZIBUXqzLMrv2Hp1iN4e1l45e6udL2qldmRRESkgVJBkTrxxhcHeW39QQBmjkxiQMcIkxOJiEhDpoIil+2DbUd4avkeAP44JIHR3WNMTiQiIg2dCopcli8yjzPln9sBGJvchgn92pmcSEREGgMVFLlkGTlFPLAonQqXwY2dI/mf4Z00JVZEROqECopckoPHS7hv/mZKy10ktwth9ugueGlKrIiI1BEVFKm1I6fK+NUbX3GipJxOkUG8dk83rD6aEisiInVHBUVq5Xixk3ve+IqjRWdoGxbAwvE9CWymKbEiIlK3VFDkohWVVXDvm5s5+N0I+8XjexHawmp2LBERaYRUUOSinC6vvr7OnlwHoS2sLP51L6Ja+psdS0REGikVFLkgZ6WL+xelk364kKBmPiwa35O40ACzY4mISCOmgiLnVemq4vf/2MYXmSdo7ufN/HE9uToyyOxYIiLSyKmgyDlVVRk8ujSDlbvy8PP24u/3dNf1dURE5IpQQZGzMgyD//33bv6VnoO3l4X/u/M6ro8PNTuWiIg0ESooclYvfprJ/I2HAJg1qjNDE+3mBhIRkSZFBUV+4o0vDvLSZ5kAPDniGkZ1izY5kYiINDW1Lijr16/npptuIioqCovFwvvvv1/jccMwmD59OlFRUfj7+9OvXz927dpVY43T6WTixImEhoYSEBDAiBEjyMnJuawXInXj3bRs95WJp6R0YExyG3MDiYhIk1TrglJaWkqXLl2YM2fOWR+fNWsWs2fPZs6cOaSlpWG32xk8eDDFxcXuNampqSxbtowlS5awYcMGSkpKGD58OC6X69JfiVy25TtyeXTpDgB+e0NbHurf3uREIiLSVFkMwzAu+ZstFpYtW8Ytt9wCVB89iYqKIjU1lUceeQSoPloSERHBs88+y/33309RURFhYWEsWrSI22+/HYCjR48SExPDihUrGDJkyAV/rsPhwGazUVRURFCQPvJaF9buzec3C7dQ4TK4s2cMM25N0pWJRUSkTtXm/btOz0HJysoiLy+PlJQU931Wq5W+ffuyceNGANLT06moqKixJioqisTERPeaH3M6nTgcjho3qTtph07ywOJ0KlwGwztH8tQtKiciImKuOi0oeXl5AERERNS4PyIiwv1YXl4efn5+tGrV6pxrfmzmzJnYbDb3LSYmpi5jN2k7jxQxbl4aZyqq6J8QxuzR1+LtpXIiIiLmqpdP8fz4X9+GYVzwX+TnWzNt2jSKiorct+zs7DrL2pQdPF7CmDc3U+yspGdcMK/c3Q0/H32wS0REzFen70Z2e/WsjB8fCcnPz3cfVbHb7ZSXl1NYWHjONT9mtVoJCgqqcZPLc8xxhnvmbqagtJzE1kHMHdMdfz9vs2OJiIgAdVxQ4uLisNvtrF692n1feXk569atIzk5GYBu3brh6+tbY01ubi47d+50r5H6VXS6gnvnbubIqTLiQgOYf19PApv5mh1LRETEzae231BSUsL+/fvdX2dlZbFt2zaCg4O56qqrSE1NZcaMGcTHxxMfH8+MGTNo3rw5d911FwA2m43x48czefJkQkJCCA4OZsqUKSQlJTFo0KC6e2VyVmXlLsYvSGPvsWLCA60sHNeT0BZWs2OJiIjUUOuCsmXLFvr37+/+etKkSQCMGTOG+fPnM3XqVMrKypgwYQKFhYX06tWLVatWERgY6P6eF154AR8fH0aPHk1ZWRkDBw5k/vz5eHvrVwz1qcJVxcNvf82Ww4UENfNh4fiexAQ3NzuWiIjIT1zWHBSzaA5K7RmGwZR/7uC9r3Ow+nix+Ne96NEm2OxYIiLShJg2B0U81zMff8N7X1dfmfjlu7qqnIiIiEdTQWkCXlt3gNfWHwTg2VGdGdTp7J+WEhER8RQqKI3cP7dkM/PjbwB47Bcd+aWuTCwiIg2ACkoj9unuYzy6NAOA+29oy29vaGdyIhERkYujgtJIpR06yUNvf42rymBU12geHdbR7EgiIiIXTQWlEfomz8H4+Wk4K6sY2DGcZ0bp4n8iItKwqKA0MtknT3Pv3M04zlTSPbYVc+7qiq+3/phFRKRh0TtXI3KixMm9b24mv9hJQkQgc8f00PV1RESkQVJBaSSKz1Qwdt5msk6U0rqlPwvH98TWXNfXERGRhkkFpRFwVrq4f1E6O484CA7wY9H4nkQENTM7loiIyCVTQWngXFUGk97ZzsYDBQT4eTP/vh60DWthdiwREZHLooLSgBmGwZ//vZvlGbn4eXvx93u70zm6pdmxRERELpsKSgM2d0MW8zceAuAvo7vws/ah5gYSERGpIyooDdRH24/y1PI9ADz+i6sZ0SXK5EQiIiJ1RwWlAdp0sIDJ724HYGxyG3798ziTE4mIiNQtFZQGZt+xYn67cAvlriqGXmPnieGdNCVWREQaHRWUBiSv6Axj3/zvlNgX77gWby+VExERaXxUUBoIx3eD2I4WnaFtWACv39udZr6aEisiIo2TCkoDUF5ZxYOL0/kmr5iwQCsL7utJqwA/s2OJiIjUGxUUD2cYBo+8t4P/7C+guZ8388b2ICa4udmxRERE6pUKiod77pO9LNt6BG8vC6/c3ZXE1jazI4mIiNQ7FRQPtmjTYV5ZewCAmSOT6JcQbnIiERGRK0MFxUOt2pXHnz7YCcAfBnVgdPcYkxOJiIhcOSooHujrbwv53ZKtVBlwR48YfjewvdmRRERErigVFA+TdaKUXy/YwpmKKvonhPHULYkaxCYiIk2OCooHOVHiZOy8zZwsLSeptY05d3XFx1t/RCIi0vTo3c9DnC6vZPz8NA4XnCYm2J83x/YgwOpjdiwRERFTqKB4gEpXFRPf3sr2nCJaNfdlwX09CQu0mh1LRETENCooJjMMgyc+2Mln3+Rj9fHijTE9aBvWwuxYIiIiplJBMdnLa/bzj83ZWCzw0h3X0S22ldmRRERETKeCYqJ/pefwl1X7AHhyxDUMTbSbnEhERMQzqKCYZP2+4zz63g4AHujbjnv7tDE3kIiIiAep84Iyffp0LBZLjZvd/t8jA4ZhMH36dKKiovD396dfv37s2rWrrmN4tF1Hi3hwcTqVVQY3XxvF1CEJZkcSERHxKPVyBOWaa64hNzfXfcvIyHA/NmvWLGbPns2cOXNIS0vDbrczePBgiouL6yOKx8kpPM3YeWmUlrvo0zaEWb/sjJeXBrGJiIj8UL0UFB8fH+x2u/sWFhYGVB89efHFF3n88ccZOXIkiYmJLFiwgNOnT/P222/XRxSPcup0OWPnpXG82ElHeyCv3dsNq4+32bFEREQ8Tr0UlMzMTKKiooiLi+OOO+7g4MGDAGRlZZGXl0dKSop7rdVqpW/fvmzcuPGcz+d0OnE4HDVuDc2ZChe/XZjO/vwSIm3NmHdfD4Ka+ZodS0RExCPVeUHp1asXCxcu5JNPPuH1118nLy+P5ORkCgoKyMvLAyAiIqLG90RERLgfO5uZM2dis9nct5iYhnVl36oqg8nvbmfzoZMENvNh/n09ibT5mx1LRETEY9V5QRk2bBijRo0iKSmJQYMGsXz5cgAWLFjgXvPji98ZhnHeC+JNmzaNoqIi9y07O7uuY9erp1fsYXlGLr7eFl67pxsJ9kCzI4mIiHi0ev+YcUBAAElJSWRmZro/zfPjoyX5+fk/OaryQ1arlaCgoBq3huKNLw4yd0MWAH+5rQvJ7UJNTiQiIuL56r2gOJ1O9uzZQ2RkJHFxcdjtdlavXu1+vLy8nHXr1pGcnFzfUa645TtyeXrFHgCmDevIzde2NjmRiIhIw1Dnl8udMmUKN910E1dddRX5+fk89dRTOBwOxowZg8ViITU1lRkzZhAfH098fDwzZsygefPm3HXXXXUdxVRfHSzgD+9swzBgTJ9YfntDW7MjiYiINBh1XlBycnK48847OXHiBGFhYfTu3ZtNmzYRGxsLwNSpUykrK2PChAkUFhbSq1cvVq1aRWBg4zkvI/NYMb9ZuIVyVxVDrongf2665rzn2IiIiEhNFsMwDLND1JbD4cBms1FUVORx56Mcc5xh5CsbOXKqjG6xrXjr171o5qtZJyIiIrV5/9a1eOpQ8ZkKxs5L48ipMtqGBvDGvd1VTkRERC6BCkodqXBVMeGtr9mT6yC0hR8LxvWkVYCf2bFEREQaJBWUOmAYBtOWZvBF5gma+3kzb2xPYoKbmx1LRESkwVJBqQMvfZbJv9Jz8Pay8PJdXUmKtpkdSUREpEFTQblM727J5sVPMwH4882J9O8YbnIiERGRhk8F5TKs33ecx5ZmAPBQ/3bc1esqkxOJiIg0Diool2j3UQcT3vqayiqDW66NYkpKgtmRREREGg0VlEtw9FQZ983fTImzkj5tQ5j1yy4axCYiIlKHVFBqyXGmgvvmpXHM4SQ+vAV/u6cbfj7ajCIiInVJ76y1UF5ZxYOL09l7rJjwQCvzx/XE5u9rdiwREZFGRwXlIhmGwaNLd/Cf/QUE+Hnz5tgetG7pb3YsERGRRkkF5SK9sHofS78+Uj3r5O6uJLbWrBMREZH6ooJyEd5J+5b/+3w/AE/fkki/BM06ERERqU8qKBewbt9xHlu2E4CJA9pzR0/NOhEREalvKijnsfNIERMWp+OqMhh5XWsmDe5gdiQREZEmQQXlHI6cKmPc/DRKy10ktwvhmVGdNetERETkClFBOYuisgrum7eZ/GInCRGBmnUiIiJyheld90fKK6t4YFE6+46VEBFkZd59PQhqplknIiIiV5IKyg8YhsEj7+3gy4P/nXUSpVknIiIiV5wKyg+syMhj2dbqWSev/Kob10Rp1omIiIgZfMwO4El+kWRnQr92tAkJoG+HMLPjiIiINFkqKD9gsViYOrSj2TFERESaPP2KR0RERDyOCoqIiIh4HBUUERER8TgqKCIiIuJxVFBERETE46igiIiIiMdRQRERERGPo4IiIiIiHkcFRURERDyOCoqIiIh4HBUUERER8TgqKCIiIuJxVFBERETE4zTIqxkbhgGAw+EwOYmIiIhcrO/ft79/Hz+fBllQiouLAYiJiTE5iYiIiNRWcXExNpvtvGssxsXUGA9TVVXF0aNHCQwMxGKx1OlzOxwOYmJiyM7OJigoqE6fu7HRtrp42lYXT9vq4mlb1Y6218Wrr21lGAbFxcVERUXh5XX+s0wa5BEULy8voqOj6/VnBAUFaQe+SNpWF0/b6uJpW108bava0fa6ePWxrS505OR7OklWREREPI4KioiIiHgcFZQfsVqt/OlPf8JqtZodxeNpW108bauLp2118bStakfb6+J5wrZqkCfJioiISOOmIygiIiLicVRQRERExOOooIiIiIjHUUERERERj6OC8gOvvPIKcXFxNGvWjG7duvHFF1+YHckjTZ8+HYvFUuNmt9vNjuUR1q9fz0033URUVBQWi4X333+/xuOGYTB9+nSioqLw9/enX79+7Nq1y5ywJrvQtho7duxP9rPevXubE9ZkM2fOpEePHgQGBhIeHs4tt9zC3r17a6zRvlXtYraV9q1qr776Kp07d3YPY+vTpw8ff/yx+3Gz9ykVlO+88847pKam8vjjj7N161Z+/vOfM2zYML799luzo3mka665htzcXPctIyPD7EgeobS0lC5dujBnzpyzPj5r1ixmz57NnDlzSEtLw263M3jwYPf1pZqSC20rgKFDh9bYz1asWHEFE3qOdevW8dBDD7Fp0yZWr15NZWUlKSkplJaWutdo36p2MdsKtG8BREdH88wzz7Blyxa2bNnCgAEDuPnmm90lxPR9yhDDMAyjZ8+exgMPPFDjvo4dOxqPPvqoSYk815/+9CejS5cuZsfweICxbNky99dVVVWG3W43nnnmGfd9Z86cMWw2m/G3v/3NhISe48fbyjAMY8yYMcbNN99sSh5Pl5+fbwDGunXrDMPQvnU+P95WhqF963xatWplvPHGGx6xT+kIClBeXk56ejopKSk17k9JSWHjxo0mpfJsmZmZREVFERcXxx133MHBgwfNjuTxsrKyyMvLq7GfWa1W+vbtq/3sHNauXUt4eDgdOnTgN7/5Dfn5+WZH8ghFRUUABAcHA9q3zufH2+p72rdqcrlcLFmyhNLSUvr06eMR+5QKCnDixAlcLhcRERE17o+IiCAvL8+kVJ6rV69eLFy4kE8++YTXX3+dvLw8kpOTKSgoMDuaR/t+X9J+dnGGDRvGW2+9xeeff87zzz9PWloaAwYMwOl0mh3NVIZhMGnSJK6//noSExMB7VvncrZtBdq3figjI4MWLVpgtVp54IEHWLZsGZ06dfKIfapBXs24vlgslhpfG4bxk/uk+n/u7yUlJdGnTx/atWvHggULmDRpkonJGgbtZxfn9ttvd/93YmIi3bt3JzY2luXLlzNy5EgTk5nr4YcfZseOHWzYsOEnj2nfqulc20r71n8lJCSwbds2Tp06xXvvvceYMWNYt26d+3Ez9ykdQQFCQ0Px9vb+SSvMz8//SXuUnwoICCApKYnMzEyzo3i07z/ppP3s0kRGRhIbG9uk97OJEyfy4YcfsmbNGqKjo933a9/6qXNtq7NpyvuWn58f7du3p3v37sycOZMuXbrw0ksvecQ+pYJC9R9Qt27dWL16dY37V69eTXJyskmpGg6n08mePXuIjIw0O4pHi4uLw26319jPysvLWbdunfazi1BQUEB2dnaT3M8Mw+Dhhx9m6dKlfP7558TFxdV4XPvWf11oW51NU963fswwDJxOp2fsU1fkVNwGYMmSJYavr68xd+5cY/fu3UZqaqoREBBgHDp0yOxoHmfy5MnG2rVrjYMHDxqbNm0yhg8fbgQGBmpbGYZRXFxsbN261di6dasBGLNnzza2bt1qHD582DAMw3jmmWcMm81mLF261MjIyDDuvPNOIzIy0nA4HCYnv/LOt62Ki4uNyZMnGxs3bjSysrKMNWvWGH369DFat27dJLfVgw8+aNhsNmPt2rVGbm6u+3b69Gn3Gu1b1S60rbRv/de0adOM9evXG1lZWcaOHTuMxx57zPDy8jJWrVplGIb5+5QKyg+8/PLLRmxsrOHn52d07dq1xsfS5L9uv/12IzIy0vD19TWioqKMkSNHGrt27TI7lkdYs2aNAfzkNmbMGMMwqj8O+qc//cmw2+2G1Wo1brjhBiMjI8Pc0CY537Y6ffq0kZKSYoSFhRm+vr7GVVddZYwZM8b49ttvzY5tirNtJ8CYN2+ee432rWoX2lbat/5r3Lhx7ve8sLAwY+DAge5yYhjm71MWwzCMK3OsRkREROTi6BwUERER8TgqKCIiIuJxVFBERETE46igiIiIiMdRQRERERGPo4IiIiIiHkcFRURERDyOCoqIiIh4HBUUERER8TgqKCIiIuJxVFBERETE46igiIiIiMf5/33dkZBCClPqAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(len(episode_return)),np.cumsum(episode_return))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stable Baseline3 DQN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///gpfs/data/oermannlab/users/hz2212/DS-GA%203001-007/disneyenv\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hInstalling collected packages: disneyenv\n",
      "  Attempting uninstall: disneyenv\n",
      "    Found existing installation: disneyenv 0.0.1\n",
      "    Uninstalling disneyenv-0.0.1:\n",
      "      Successfully uninstalled disneyenv-0.0.1\n",
      "  Running setup.py develop for disneyenv\n",
      "Successfully installed disneyenv-0.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -e disneyenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import disneyenv\n",
    "import torch\n",
    "\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CallbackList, ProgressBarCallback\n",
    "from stable_baselines3 import PPO, DQN\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "\n",
    "import wandb\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_env(i):\n",
    "    train_env = gym.make(\"disneyenv/Disney-v0\", train=True)\n",
    "    train_env = Monitor(train_env, filename=\"./monitor_logs/train\", info_keywords=(\"current_date\",))\n",
    "    return train_env\n",
    "\n",
    "def get_eval_env(i):\n",
    "    eval_env = gym.make(\"disneyenv/Disney-v0\", train=False)\n",
    "    eval_env = Monitor(eval_env, filename=\"./monitor_logs/eval\", info_keywords=(\"current_date\",))\n",
    "    return eval_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Using cpu device\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Using cpu device\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_env = DummyVecEnv([lambda : get_train_env(x) for x in range(2)])\n",
    "eval_env = DummyVecEnv([lambda : get_eval_env(x) for x in range(2)])\n",
    "\n",
    "# run = wandb.init(project=\"disney_rl\", name=\"disney_run\", sync_tensorboard=True)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = PPO(\"MultiInputPolicy\", train_env, verbose=1, device=\"cpu\")\n",
    "# agent = PPO(\"MultiInputPolicy\",env,verbose=1, device=device, batch_size=1024, n_steps=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_callback = EvalCallback(eval_env, best_model_save_path=\"./eval_logs/\",\n",
    "             log_path=\"./eval_logs/\", eval_freq=2, n_eval_episodes=15)\n",
    "\n",
    "# wandb_callback = WandbCallback(\n",
    "#             model_save_path=f\"models/{run.id}\",\n",
    "#             model_save_freq=100,\n",
    "#             verbose=2,\n",
    "#         )\n",
    "\n",
    "pb_callback = ProgressBarCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff79ed603dfd4630bdc7e6b79bb87905",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCallbackList\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43meval_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpb_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:308\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    301\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    306\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    307\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:250\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    247\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_start(\u001b[38;5;28mlocals\u001b[39m(), \u001b[38;5;28mglobals\u001b[39m())\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 250\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m continue_training \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:184\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n\u001b[1;32m    183\u001b[0m callback\u001b[38;5;241m.\u001b[39mupdate_locals(\u001b[38;5;28mlocals\u001b[39m())\n\u001b[0;32m--> 184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcallback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_info_buffer(infos)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/callbacks.py:104\u001b[0m, in \u001b[0;36mBaseCallback.on_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_calls \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mnum_timesteps\n\u001b[0;32m--> 104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_on_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/callbacks.py:208\u001b[0m, in \u001b[0;36mCallbackList._on_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    205\u001b[0m continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;66;03m# Return False (stop training) if at least one callback returns False\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[43mcallback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m continue_training\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m continue_training\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/callbacks.py:104\u001b[0m, in \u001b[0;36mBaseCallback.on_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_calls \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mnum_timesteps\n\u001b[0;32m--> 104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_on_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/callbacks.py:449\u001b[0m, in \u001b[0;36mEvalCallback._on_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;66;03m# Reset success rate buffer\u001b[39;00m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_success_buffer \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 449\u001b[0m episode_rewards, episode_lengths \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_policy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_eval_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_eval_episodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_episode_rewards\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwarn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_success_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluations_timesteps\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:89\u001b[0m, in \u001b[0;36mevaluate_policy\u001b[0;34m(model, env, n_eval_episodes, deterministic, render, callback, reward_threshold, return_episode_rewards, warn)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m (episode_counts \u001b[38;5;241m<\u001b[39m episode_count_targets)\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m     88\u001b[0m     actions, states \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(observations, state\u001b[38;5;241m=\u001b[39mstates, episode_start\u001b[38;5;241m=\u001b[39mepisode_starts, deterministic\u001b[38;5;241m=\u001b[39mdeterministic)\n\u001b[0;32m---> 89\u001b[0m     observations, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m     current_rewards \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m rewards\n\u001b[1;32m     91\u001b[0m     current_lengths \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:163\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \n\u001b[1;32m    159\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:54\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[0;32m---> 54\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx]:\n\u001b[1;32m     58\u001b[0m             \u001b[38;5;66;03m# save final observation where user can get it, then reset\u001b[39;00m\n\u001b[1;32m     59\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mterminal_observation\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m obs\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/monitor.py:95\u001b[0m, in \u001b[0;36mMonitor.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneeds_reset:\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to step environment that needs reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 95\u001b[0m observation, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards\u001b[38;5;241m.\u001b[39mappend(reward)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n",
      "File \u001b[0;32m~/.conda/envs/ds3001-007/lib/python3.10/site-packages/gym/wrappers/time_limit.py:18\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 18\u001b[0m     observation, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/DS-GA 3001-007/disneyenv/disneyenv/envs/disney.py:268\u001b[0m, in \u001b[0;36mDisneyEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_land \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mridesinfo\u001b[38;5;241m.\u001b[39miloc[action]\u001b[38;5;241m.\u001b[39mlandID\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m# get next observation\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_observation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m info \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcurrent_date\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_date}\n\u001b[1;32m    272\u001b[0m \u001b[38;5;66;03m# the visit is over if the time is after 10:00 pm\u001b[39;00m\n",
      "File \u001b[0;32m~/DS-GA 3001-007/disneyenv/disneyenv/envs/disney.py:93\u001b[0m, in \u001b[0;36mDisneyEnv.__get_observation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__get_observation\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     88\u001b[0m \n\u001b[1;32m     89\u001b[0m     \u001b[38;5;66;03m# for each attraction, return its wait time\u001b[39;00m\n\u001b[1;32m     90\u001b[0m     input_waitTime_index \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mMultiIndex\u001b[38;5;241m.\u001b[39mfrom_arrays(\n\u001b[1;32m     91\u001b[0m         [np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrides), np\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_time, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrides))])\n\u001b[0;32m---> 93\u001b[0m     waitTime, operationStatus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve_closest_prior_info\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_waitTime_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitTime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwaitTime\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;66;03m# weather\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     rainStatus, feelsLikeF \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve_closest_prior_info(\n\u001b[1;32m     98\u001b[0m         [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_time], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweather_today, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweather\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/DS-GA 3001-007/disneyenv/disneyenv/envs/disney.py:124\u001b[0m, in \u001b[0;36mDisneyEnv.retrieve_closest_prior_info\u001b[0;34m(self, input_index, target_df, event_type)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mretrieve_closest_prior_info\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_index, target_df, event_type: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    122\u001b[0m \n\u001b[1;32m    123\u001b[0m     \u001b[38;5;66;03m# get iloc of the target indexes\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m     target_ilocs \u001b[38;5;241m=\u001b[39m \u001b[43mtarget_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mffill\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;66;03m# index of valid ilocs\u001b[39;00m\n\u001b[1;32m    126\u001b[0m     valid_targets \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(target_ilocs \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/ds3001-007/lib/python3.10/site-packages/pandas/core/indexes/base.py:3973\u001b[0m, in \u001b[0;36mIndex.get_indexer\u001b[0;34m(self, target, method, limit, tolerance)\u001b[0m\n\u001b[1;32m   3968\u001b[0m     target \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   3969\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m this\u001b[38;5;241m.\u001b[39m_get_indexer(\n\u001b[1;32m   3970\u001b[0m         target, method\u001b[38;5;241m=\u001b[39mmethod, limit\u001b[38;5;241m=\u001b[39mlimit, tolerance\u001b[38;5;241m=\u001b[39mtolerance\n\u001b[1;32m   3971\u001b[0m     )\n\u001b[0;32m-> 3973\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/ds3001-007/lib/python3.10/site-packages/pandas/core/indexes/base.py:3986\u001b[0m, in \u001b[0;36mIndex._get_indexer\u001b[0;34m(self, target, method, limit, tolerance)\u001b[0m\n\u001b[1;32m   3983\u001b[0m     tolerance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_tolerance(tolerance, target)\n\u001b[1;32m   3985\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpad\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackfill\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m-> 3986\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_fill_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3987\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnearest\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   3988\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_nearest_indexer(target, limit, tolerance)\n",
      "File \u001b[0;32m~/.conda/envs/ds3001-007/lib/python3.10/site-packages/pandas/core/indexes/base.py:4083\u001b[0m, in \u001b[0;36mIndex._get_fill_indexer\u001b[0;34m(self, target, method, limit, tolerance)\u001b[0m\n\u001b[1;32m   4078\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_multi:\n\u001b[1;32m   4079\u001b[0m     \u001b[38;5;66;03m# TODO: get_indexer_with_fill docstring says values must be _sorted_\u001b[39;00m\n\u001b[1;32m   4080\u001b[0m     \u001b[38;5;66;03m#  but that doesn't appear to be enforced\u001b[39;00m\n\u001b[1;32m   4081\u001b[0m     \u001b[38;5;66;03m# error: \"IndexEngine\" has no attribute \"get_indexer_with_fill\"\u001b[39;00m\n\u001b[1;32m   4082\u001b[0m     engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\n\u001b[0;32m-> 4083\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_indexer_with_fill\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[union-attr]\u001b[39;49;00m\n\u001b[1;32m   4084\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\n\u001b[1;32m   4085\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4087\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_monotonic_increasing \u001b[38;5;129;01mand\u001b[39;00m target\u001b[38;5;241m.\u001b[39mis_monotonic_increasing:\n\u001b[1;32m   4088\u001b[0m     target_values \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39m_get_engine_target()\n",
      "File \u001b[0;32m~/.conda/envs/ds3001-007/lib/python3.10/site-packages/pandas/_libs/index.pyx:766\u001b[0m, in \u001b[0;36mpandas._libs.index.BaseMultiIndexCodesEngine.get_indexer_with_fill\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m<__array_function__ internals>:177\u001b[0m, in \u001b[0;36margsort\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=10000, callback=CallbackList([eval_callback, pb_callback]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">A new day! Today is 2018-08-20\n",
       "</pre>\n"
      ],
      "text/plain": [
       "A new day! Today is 2018-08-20\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">at 2018-08-20 08:07:20 agent go to ride 45 and get reward -1.4666666666666668\n",
       "</pre>\n"
      ],
      "text/plain": [
       "at 2018-08-20 08:07:20 agent go to ride 45 and get reward -1.4666666666666668\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">at 2018-08-20 08:08:20 agent go to ride 47 and get reward -0.2\n",
       "</pre>\n"
      ],
      "text/plain": [
       "at 2018-08-20 08:08:20 agent go to ride 47 and get reward -0.2\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">at 2018-08-20 08:51:17 agent go to ride 67 and get reward -2.290000000000001\n",
       "</pre>\n"
      ],
      "text/plain": [
       "at 2018-08-20 08:51:17 agent go to ride 67 and get reward -2.290000000000001\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">at 2018-08-20 09:23:26 agent go to ride 56 and get reward -0.8300000000000001\n",
       "</pre>\n"
      ],
      "text/plain": [
       "at 2018-08-20 09:23:26 agent go to ride 56 and get reward -0.8300000000000001\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">at 2018-08-20 09:27:13 agent go to ride 24 and get reward -0.7566666666666667\n",
       "</pre>\n"
      ],
      "text/plain": [
       "at 2018-08-20 09:27:13 agent go to ride 24 and get reward -0.7566666666666667\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">at 2018-08-20 09:46:06 agent go to ride 81 and get reward -3.776666666666667\n",
       "</pre>\n"
      ],
      "text/plain": [
       "at 2018-08-20 09:46:06 agent go to ride 81 and get reward -3.776666666666667\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">at 2018-08-20 09:54:36 agent go to ride 77 and get reward 9.8\n",
       "</pre>\n"
      ],
      "text/plain": [
       "at 2018-08-20 09:54:36 agent go to ride 77 and get reward 9.8\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">at 2018-08-20 11:33:19 agent go to ride 93 and get reward 1.456666666666667\n",
       "</pre>\n"
      ],
      "text/plain": [
       "at 2018-08-20 11:33:19 agent go to ride 93 and get reward 1.456666666666667\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">at 2018-08-20 11:46:14 agent go to ride 91 and get reward -2.5833333333333335\n",
       "</pre>\n"
      ],
      "text/plain": [
       "at 2018-08-20 11:46:14 agent go to ride 91 and get reward -2.5833333333333335\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">at 2018-08-20 12:25:55 agent go to ride 94 and get reward 7.463333333333333\n",
       "</pre>\n"
      ],
      "text/plain": [
       "at 2018-08-20 12:25:55 agent go to ride 94 and get reward 7.463333333333333\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">at 2018-08-20 13:11:55 agent go to ride 72 and get reward -1.8000000000000007\n",
       "</pre>\n"
      ],
      "text/plain": [
       "at 2018-08-20 13:11:55 agent go to ride 72 and get reward -1.8000000000000007\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">at 2018-08-20 14:52:08 agent go to ride 15 and get reward 13.356666666666666\n",
       "</pre>\n"
      ],
      "text/plain": [
       "at 2018-08-20 14:52:08 agent go to ride 15 and get reward 13.356666666666666\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">at 2018-08-20 15:42:38 agent go to ride 11 and get reward 19.8\n",
       "</pre>\n"
      ],
      "text/plain": [
       "at 2018-08-20 15:42:38 agent go to ride 11 and get reward 19.8\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">at 2018-08-20 16:14:20 agent go to ride 87 and get reward 7.66\n",
       "</pre>\n"
      ],
      "text/plain": [
       "at 2018-08-20 16:14:20 agent go to ride 87 and get reward 7.66\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">at 2018-08-20 17:40:42 agent go to ride 80 and get reward 1.5266666666666664\n",
       "</pre>\n"
      ],
      "text/plain": [
       "at 2018-08-20 17:40:42 agent go to ride 80 and get reward 1.5266666666666664\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">at 2018-08-20 18:41:33 agent go to ride 9 and get reward 6.129999999999999\n",
       "</pre>\n"
      ],
      "text/plain": [
       "at 2018-08-20 18:41:33 agent go to ride 9 and get reward 6.129999999999999\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">at 2018-08-20 19:18:53 agent go to ride 52 and get reward 3.333333333333333\n",
       "</pre>\n"
      ],
      "text/plain": [
       "at 2018-08-20 19:18:53 agent go to ride 52 and get reward 3.333333333333333\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">at 2018-08-20 20:48:26 agent go to ride 88 and get reward 1.9899999999999998\n",
       "</pre>\n"
      ],
      "text/plain": [
       "at 2018-08-20 20:48:26 agent go to ride 88 and get reward 1.9899999999999998\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">at 2018-08-20 21:32:24 agent go to ride 52 and get reward -2.9933333333333336\n",
       "</pre>\n"
      ],
      "text/plain": [
       "at 2018-08-20 21:32:24 agent go to ride 52 and get reward -2.9933333333333336\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">The day is over! The reward is 52.806666666666665\n",
       "</pre>\n"
      ],
      "text/plain": [
       "The day is over! The reward is 52.806666666666665\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">at 2018-08-20 22:48:28 agent go to ride 93 and get reward -3.0133333333333336\n",
       "</pre>\n"
      ],
      "text/plain": [
       "at 2018-08-20 22:48:28 agent go to ride 93 and get reward -3.0133333333333336\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">[-1.4666666666666668, -0.2, -2.290000000000001, -0.8300000000000001, -0.7566666666666667, -3.776666666666667, 9.8, \n",
       "1.456666666666667, -2.5833333333333335, 7.463333333333333, -1.8000000000000007, 13.356666666666666, 19.8, 7.66, \n",
       "1.5266666666666664, 6.129999999999999, 3.333333333333333, 1.9899999999999998, -2.9933333333333336, \n",
       "-3.0133333333333336]\n",
       "</pre>\n"
      ],
      "text/plain": [
       "[-1.4666666666666668, -0.2, -2.290000000000001, -0.8300000000000001, -0.7566666666666667, -3.776666666666667, 9.8, \n",
       "1.456666666666667, -2.5833333333333335, 7.463333333333333, -1.8000000000000007, 13.356666666666666, 19.8, 7.66, \n",
       "1.5266666666666664, 6.129999999999999, 3.333333333333333, 1.9899999999999998, -2.9933333333333336, \n",
       "-3.0133333333333336]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "return_val = []\n",
    "while True:\n",
    "    action, _states = agent.predict(obs, deterministic=False)\n",
    "    #action = env.action_space.sample()\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    print(\"at\", env.current_time, \"agent go to ride\", action, \"and get reward\", reward)\n",
    "    return_val += [reward]\n",
    "    \n",
    "    if done:\n",
    "        print(return_val)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
