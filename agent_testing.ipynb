{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "忽略Keras RL的部分。Stable-baselines在下面  \n",
    "这个现在用的是gym 0.26.2, 在monitor.py里有一个return的问题可能需要更改return的value数量才能兼容  \n",
    "PPO感觉挺好的，在只120步的时候可以超过random选择的return。DQN在训练120步的时候不行。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras RL DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///gpfs/data/oermannlab/users/hz2212/DS-GA%203001-007/disneyenv\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hInstalling collected packages: disneyenv\n",
      "  Running setup.py develop for disneyenv\n",
      "Successfully installed disneyenv-0.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -e disneyenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/data/oermannlab/users/hz2212/DS-GA 3001-007/disneyenv/disneyenv/envs/disney.py:42: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  self.waittime = pd.read_csv(\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "action space does not inherit from `gymnasium.spaces.Space`, actual type: <class 'gym.spaces.discrete.Discrete'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdisneyenv\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdisneyenv/Disney-v0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/ds3001-007/lib/python3.10/site-packages/gymnasium/envs/registration.py:669\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, autoreset, apply_api_compatibility, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[38;5;66;03m# Run the environment checker as the lowest level wrapper\u001b[39;00m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m disable_env_checker \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m    667\u001b[0m     disable_env_checker \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m spec_\u001b[38;5;241m.\u001b[39mdisable_env_checker \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    668\u001b[0m ):\n\u001b[0;32m--> 669\u001b[0m     env \u001b[38;5;241m=\u001b[39m \u001b[43mPassiveEnvChecker\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;66;03m# Add the order enforcing wrapper\u001b[39;00m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec_\u001b[38;5;241m.\u001b[39morder_enforce:\n",
      "File \u001b[0;32m~/.conda/envs/ds3001-007/lib/python3.10/site-packages/gymnasium/wrappers/env_checker.py:23\u001b[0m, in \u001b[0;36mPassiveEnvChecker.__init__\u001b[0;34m(self, env)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(env)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\n\u001b[1;32m     21\u001b[0m     env, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction_space\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     22\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe environment must specify an action space. https://gymnasium.farama.org/content/environment_creation/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 23\u001b[0m \u001b[43mcheck_action_space\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_space\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\n\u001b[1;32m     25\u001b[0m     env, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobservation_space\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     26\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe environment must specify an observation space. https://gymnasium.farama.org/content/environment_creation/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     27\u001b[0m check_observation_space(env\u001b[38;5;241m.\u001b[39mobservation_space)\n",
      "File \u001b[0;32m~/.conda/envs/ds3001-007/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:74\u001b[0m, in \u001b[0;36mcheck_space\u001b[0;34m(space, space_type, check_box_space_fn)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m\"\"\"A passive check of the environment action space that should not affect the environment.\"\"\"\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(space, spaces\u001b[38;5;241m.\u001b[39mSpace):\n\u001b[0;32m---> 74\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspace_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m space does not inherit from `gymnasium.spaces.Space`, actual type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(space)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     76\u001b[0m     )\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(space, spaces\u001b[38;5;241m.\u001b[39mBox):\n\u001b[1;32m     79\u001b[0m     check_box_space_fn(space)\n",
      "\u001b[0;31mAssertionError\u001b[0m: action space does not inherit from `gymnasium.spaces.Space`, actual type: <class 'gym.spaces.discrete.Discrete'>"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import disneyenv\n",
    "\n",
    "env = gym.make('disneyenv/Disney-v0') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten,Input,Dropout,Reshape\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents import DQNAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape, output_shape):\n",
    "    model = Sequential([\n",
    "        Dense(512,activation=\"relu\",input_shape = (1,input_shape)),\n",
    "        Dropout(0.2),\n",
    "        Flatten(),\n",
    "        Dense(256,activation = \"relu\"),\n",
    "        Dropout(0.2),\n",
    "        Dense(128,activation = \"relu\"),\n",
    "        Dense(64,activation = \"relu\"),\n",
    "        Dense(output_shape,activation = \"linear\"),\n",
    "        Flatten()\n",
    "    ])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.2, nb_steps=10000)\n",
    "    memory = SequentialMemory(limit=2000,window_length = 1)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy,\n",
    "                  enable_dueling_network=True, dueling_type='avg',\n",
    "                  nb_actions=actions, nb_steps_warmup=10)\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 1, 512)            118784    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1, 512)            0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 106)               6890      \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 106)               0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 298,154\n",
      "Trainable params: 298,154\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(env.observation_space.n,env.action_space.n)\n",
    "dqn_agent = build_agent(model,env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_agent.compile(Adam(learning_rate=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 1000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\82751\\anaconda3\\envs\\my_environment\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gymnasium versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\82751\\anaconda3\\envs\\my_environment\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:187: UserWarning: \u001b[33mWARN: Future gymnasium versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A new day! Today is 2018-7-26\n",
      "Interval 1 (0 steps performed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\82751\\anaconda3\\envs\\my_environment\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:195: UserWarning: \u001b[33mWARN: The result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `<class 'numpy.ndarray'>`\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\82751\\anaconda3\\envs\\my_environment\\lib\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    1/10000 [..............................] - ETA: 1:21:38 - reward: 1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\82751\\anaconda3\\envs\\my_environment\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:133: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method should be an int or np.int64, actual type: <class 'numpy.ndarray'>\u001b[0m\n",
      "  logger.warn(f\"{pre} should be an int or np.int64, actual type: {type(obs)}\")\n",
      "c:\\Users\\82751\\anaconda3\\envs\\my_environment\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   11/10000 [..............................] - ETA: 46:40 - reward: -35.2727"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\82751\\anaconda3\\envs\\my_environment\\lib\\site-packages\\rl\\memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   72/10000 [..............................] - ETA: 55:26 - reward: -33.9792A new day! Today is 2017-7-24\n",
      "  119/10000 [..............................] - ETA: 54:14 - reward: -27.1176A new day! Today is 2017-10-23\n",
      "  191/10000 [..............................] - ETA: 53:18 - reward: -30.8901A new day! Today is 2018-4-12\n",
      "  253/10000 [..............................] - ETA: 52:33 - reward: -30.8043A new day! Today is 2018-6-22\n",
      "  307/10000 [..............................] - ETA: 52:05 - reward: -29.1824A new day! Today is 2018-6-20\n",
      "  358/10000 [>.............................] - ETA: 51:42 - reward: -29.3142A new day! Today is 2017-8-6\n",
      "  400/10000 [>.............................] - ETA: 51:25 - reward: -28.6212A new day! Today is 2018-6-28\n",
      "  449/10000 [>.............................] - ETA: 51:04 - reward: -28.3864A new day! Today is 2018-9-12\n",
      "  517/10000 [>.............................] - ETA: 50:21 - reward: -28.9052A new day! Today is 2018-8-21\n",
      "  564/10000 [>.............................] - ETA: 50:03 - reward: -28.6835A new day! Today is 2017-12-26\n",
      "  620/10000 [>.............................] - ETA: 49:50 - reward: -27.9808A new day! Today is 2018-1-4\n",
      "  657/10000 [>.............................] - ETA: 49:42 - reward: -27.4909A new day! Today is 2018-8-20\n",
      "  712/10000 [=>............................] - ETA: 52:03 - reward: -27.5822A new day! Today is 2017-11-24\n",
      "  764/10000 [=>............................] - ETA: 51:18 - reward: -27.3043A new day! Today is 2018-1-9\n",
      "  852/10000 [=>............................] - ETA: 50:37 - reward: -28.6092A new day! Today is 2018-8-7\n",
      "  906/10000 [=>............................] - ETA: 50:10 - reward: -28.6987A new day! Today is 2017-9-15\n",
      "  978/10000 [=>............................] - ETA: 49:39 - reward: -29.1131A new day! Today is 2018-9-14\n",
      " 1000/10000 [==>...........................] - ETA: 49:28 - reward: -29.2526done, took 330.622 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27c2d54e850>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn_agent.fit(env, nb_steps = 1000, visualize = False, verbose = 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stable Baseline3 DQN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///gpfs/data/oermannlab/users/hz2212/DS-GA%203001-007/disneyenv\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hInstalling collected packages: disneyenv\n",
      "  Attempting uninstall: disneyenv\n",
      "    Found existing installation: disneyenv 0.0.1\n",
      "    Uninstalling disneyenv-0.0.1:\n",
      "      Successfully uninstalled disneyenv-0.0.1\n",
      "  Running setup.py develop for disneyenv\n",
      "Successfully installed disneyenv-0.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -e disneyenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/data/oermannlab/users/hz2212/DS-GA 3001-007/disneyenv/disneyenv/envs/disney.py:42: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  self.waittime = pd.read_csv(\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import disneyenv\n",
    "\n",
    "env = gym.make(\"disneyenv/Disney-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from stable_baselines3.common.env_checker import check_env\n",
    "# check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "\n",
      " A new day! Today is 2018-04-23\n",
      "\n",
      " A new day! Today is 2017-07-22\n",
      "\n",
      " A new day! Today is 2017-07-02\n",
      "\n",
      " A new day! Today is 2018-01-07\n",
      "\n",
      " A new day! Today is 2017-11-02\n",
      "\n",
      " A new day! Today is 2018-09-11\n",
      "\n",
      " A new day! Today is 2017-10-17\n",
      "\n",
      " A new day! Today is 2018-07-02\n",
      "\n",
      " A new day! Today is 2017-08-14\n",
      "\n",
      " A new day! Today is 2018-08-13\n",
      "\n",
      " A new day! Today is 2018-07-12\n",
      "\n",
      " A new day! Today is 2018-05-14\n",
      "\n",
      " A new day! Today is 2018-06-01\n",
      "\n",
      " A new day! Today is 2017-06-16\n",
      "\n",
      " A new day! Today is 2017-10-26\n",
      "\n",
      " A new day! Today is 2017-07-22\n",
      "\n",
      " A new day! Today is 2018-03-28\n",
      "\n",
      " A new day! Today is 2018-08-13\n",
      "\n",
      " A new day! Today is 2018-05-11\n",
      "\n",
      " A new day! Today is 2018-05-06\n",
      "\n",
      " A new day! Today is 2017-06-07\n",
      "\n",
      " A new day! Today is 2017-08-08\n",
      "\n",
      " A new day! Today is 2017-09-10\n",
      "\n",
      " A new day! Today is 2017-06-01\n",
      "\n",
      " A new day! Today is 2017-06-16\n",
      "\n",
      " A new day! Today is 2017-10-04\n",
      "\n",
      " A new day! Today is 2017-10-06\n",
      "\n",
      " A new day! Today is 2017-12-25\n",
      "\n",
      " A new day! Today is 2018-09-15\n",
      "\n",
      " A new day! Today is 2017-10-28\n",
      "\n",
      " A new day! Today is 2017-11-22\n",
      "\n",
      " A new day! Today is 2018-05-13\n",
      "\n",
      " A new day! Today is 2017-08-09\n",
      "\n",
      " A new day! Today is 2017-12-26\n",
      "\n",
      " A new day! Today is 2018-09-14\n",
      "\n",
      " A new day! Today is 2017-11-13\n",
      "\n",
      " A new day! Today is 2018-01-26\n",
      "\n",
      " A new day! Today is 2018-05-31\n",
      "\n",
      " A new day! Today is 2018-03-21\n",
      "\n",
      " A new day! Today is 2018-01-29\n",
      "\n",
      " A new day! Today is 2017-08-31\n",
      "\n",
      " A new day! Today is 2018-06-02\n",
      "\n",
      " A new day! Today is 2018-06-19\n",
      "\n",
      " A new day! Today is 2018-04-20\n",
      "\n",
      " A new day! Today is 2017-11-19\n",
      "\n",
      " A new day! Today is 2018-08-01\n",
      "\n",
      " A new day! Today is 2018-07-21\n",
      "\n",
      " A new day! Today is 2017-06-28\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 43        |\n",
      "|    ep_rew_mean     | -1.34e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 6         |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 302       |\n",
      "|    total_timesteps | 2048      |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7fbe16506fe0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from stable_baselines3 import PPO, DQN\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "agent = PPO(\"MlpPolicy\",env,verbose=1, device=device)\n",
    "agent.learn(total_timesteps=120)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " A new day! Today is 2018-07-12\n",
      "at 2018-07-12 08:35:08.425107 agent go to ride 15 and get reward 20.0\n",
      "at 2018-07-12 08:57:08.425107 agent go to ride 15 and get reward 10.0\n",
      "at 2018-07-12 09:22:26.710539 agent go to ride 90 and get reward 20.0\n",
      "at 2018-07-12 09:42:26.710539 agent go to ride 90 and get reward 10.0\n",
      "at 2018-07-12 10:09:44.995971 agent go to ride 15 and get reward 5.0\n",
      "at 2018-07-12 10:31:44.995971 agent go to ride 15 and get reward 2.5\n",
      "at 2018-07-12 10:53:44.995971 agent go to ride 15 and get reward 1.25\n",
      "at 2018-07-12 11:29:03.281403 agent go to ride 90 and get reward 5.0\n",
      "at 2018-07-12 12:04:03.281403 agent go to ride 90 and get reward 2.5\n",
      "at 2018-07-12 12:31:21.566835 agent go to ride 15 and get reward 0.625\n",
      "at 2018-07-12 12:53:21.566835 agent go to ride 15 and get reward 0.3125\n",
      "at 2018-07-12 13:43:39.852267 agent go to ride 90 and get reward 1.25\n",
      "at 2018-07-12 14:28:39.852267 agent go to ride 90 and get reward 0.625\n",
      "at 2018-07-12 15:13:39.852267 agent go to ride 90 and get reward 0.3125\n",
      "at 2018-07-12 15:53:39.852267 agent go to ride 90 and get reward 0.15625\n",
      "at 2018-07-12 16:33:39.852267 agent go to ride 90 and get reward 0.078125\n",
      "at 2018-07-12 17:00:58.137699 agent go to ride 15 and get reward 0.15625\n",
      "at 2018-07-12 17:22:58.137699 agent go to ride 15 and get reward 0.078125\n",
      "at 2018-07-12 18:03:16.423131 agent go to ride 90 and get reward 0.0390625\n",
      "at 2018-07-12 18:30:34.708563 agent go to ride 15 and get reward 0.0390625\n",
      "at 2018-07-12 18:52:34.708563 agent go to ride 15 and get reward 0.01953125\n",
      "at 2018-07-12 19:42:52.993995 agent go to ride 90 and get reward 0.01953125\n",
      "at 2018-07-12 20:10:11.279427 agent go to ride 15 and get reward 0.009765625\n",
      "at 2018-07-12 20:32:11.279427 agent go to ride 15 and get reward 0.0048828125\n",
      "at 2018-07-12 20:54:11.279427 agent go to ride 15 and get reward 0.00244140625\n",
      "at 2018-07-12 22:19:23.656078 agent go to ride 93 and get reward 1.0\n",
      "at 2018-07-12 23:40:36.032729 agent go to ride 15 and get reward 0.001220703125\n",
      "[20.0, 10.0, 20.0, 10.0, 5.0, 2.5, 1.25, 5.0, 2.5, 0.625, 0.3125, 1.25, 0.625, 0.3125, 0.15625, 0.078125, 0.15625, 0.078125, 0.0390625, 0.0390625, 0.01953125, 0.01953125, 0.009765625, 0.0048828125, 0.00244140625, 1.0, 0.001220703125]\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "return_val = []\n",
    "while True:\n",
    "    action, _states = agent.predict(obs, deterministic=True)\n",
    "    #action = env.action_space.sample()\n",
    "    obs, reward, done,info = env.step(action)\n",
    "    print(\"at\",env.current_time,\"agent go to ride\",action,\"and get reward\",reward)\n",
    "    return_val += [reward]\n",
    "\n",
    "    if done:\n",
    "        print(return_val)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
